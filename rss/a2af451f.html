<!DOCTYPE html>
<html lang="zh-CN">

<head>
  <meta name="generator" content="Hexo 6.3.0">
  <meta charset="utf-8">
  

  <meta http-equiv="x-dns-prefetch-control" content="on">
  <link rel="dns-prefetch" href="https://fastly.jsdelivr.net">
  <link rel="preconnect" href="https://fastly.jsdelivr.net" crossorigin>
  <link rel="dns-prefetch" href="//unpkg.com">

  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" content="#f8f8f8">
  <title>让机器人在“想象”中学习世界的模型来了！PI联创课题组&清华陈建宇团队联合出品 - RSSBOX</title>

  
    <meta name="description" content="Ctrl-World团队 投稿量子位 | 公众号 QbitAI这两天，Physical Intelligence（PI）联合创始人Chelsea Finn在上，对斯坦福课题组一项最新世界模型工作kuakua连续点赞。生成看起来不错的视频很容易，难的是构建一个真正对机器人有用的通用模型——它需要紧密跟随动作，还要足够准确以避免频繁幻觉。这项研究，正是她在斯坦福带领的课题组与清华大学陈建宇团队联合提">
<meta property="og:type" content="article">
<meta property="og:title" content="让机器人在“想象”中学习世界的模型来了！PI联创课题组&amp;清华陈建宇团队联合出品">
<meta property="og:url" content="https://rssbox.mhuig.top/rss/a2af451f.html">
<meta property="og:site_name" content="RSSBOX">
<meta property="og:description" content="Ctrl-World团队 投稿量子位 | 公众号 QbitAI这两天，Physical Intelligence（PI）联合创始人Chelsea Finn在上，对斯坦福课题组一项最新世界模型工作kuakua连续点赞。生成看起来不错的视频很容易，难的是构建一个真正对机器人有用的通用模型——它需要紧密跟随动作，还要足够准确以避免频繁幻觉。这项研究，正是她在斯坦福带领的课题组与清华大学陈建宇团队联合提">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-794e46050c9f65d918a95ae9e53032ed_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-9681918a582a7cd66fb7a49bb18bb6be_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-fb22bd7fc0a6ef1f7bf5a25d34caf104_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-4ca6bc540e4df3379d055cd2d28935c0_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-8b02e804023b39c7810c2a933b569371_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-6177d851f81d95dbe4d77a59ecddf0da_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-942ddd6908d4410ce70630191464c112_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-a34b5a5fdc087297b4eb55f31af8d67d_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-da33392d1d749d6862ad6365a8a2b430_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-578da789b457bde9ac3e99e74f40c080_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic4.zhimg.com/v2-10ae0ca07858faea5a4e7dd4ae7cee8f_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-da33392d1d749d6862ad6365a8a2b430_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-ce5043f89dcc98b99dcdb8d32999e454_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-3d2da30f1587219f264f74d85eb1db76_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-03e95d32b23886e23228892b48487900_1440w.jpg">
<meta property="article:published_time" content="2025-10-30T10:20:04.000Z">
<meta property="article:modified_time" content="2025-10-30T10:20:04.000Z">
<meta property="article:author" content="MHuiG">
<meta property="article:tag" content="知乎">
<meta property="article:tag" content="量子位">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-794e46050c9f65d918a95ae9e53032ed_1440w.jpg">
  
  

  <!-- feed -->
  
    <link rel="alternate" href="/atom.xml" title="RSSBOX" type="application/atom+xml">
  

  
    
<link rel="stylesheet" href="/css/main.css">

  

  

  

  


  
</head>

<body>
  




  <div class="l_body" id="start">
    <aside class="l_left" layout="post">
    


<header class="header">

<div class="logo-wrap"><a class="avatar" target="_blank" rel="noopener" href="https://mhuig.top/"><div class="bg" style="opacity:0;background-image:url(https://static.mhuig.top/gh/cdn-x/placeholder@1.0.2/avatar/round/rainbow64@3x.webp);"></div><img no-lazy class="avatar" src="/avatar.png" onerror="javascript:this.classList.add('error');this.src='https://static.mhuig.top/gh/cdn-x/placeholder@1.0.1/image/2659360.svg';"></a><a class="title" href="/"><div class="main">RSSBOX</div><div class="sub normal cap">MHuiGのRSS订阅</div><div class="sub hover cap" style="opacity:0">rssbox.mhuig.top</div></a></div>
<nav class="menu dis-select"><a class="nav-item active" href="/">RSS</a><a class="nav-item" target="_blank" rel="noopener" href="https://blog.mhuig.top/">博客</a><a class="nav-item" target="_blank" rel="noopener" href="https://mhuig.top/">关于</a></nav></header>

<div class="widgets">

<div class="widget-wrap single" id="toc"><div class="widget-header cap dis-select"><span class="name">本文目录</span></div><div class="widget-body fs14"><div class="doc-tree active"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">研究背景：机器人训练的“真实世界困境”与世界模型的破局价值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">三大创新技术，让CTRL-WORLD突破传统世界模型局限</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">第一，多视角联合预测：解决“视野盲区”，降低幻觉率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">第二，帧级动作控制：绑定动作与视觉因果，实现厘米级精准操控</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">第三，姿态条件记忆检索：给长时模拟“装稳定器”，20秒长时预演不漂移</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">实验验证：从“虚拟评估”到“策略提升”的全流程实效</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">生成质量：多指标碾压传统模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">策略评估：虚拟打分与真实表现高度对齐</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">策略优化：400条虚拟轨迹实现44.7%性能飞跃</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">研究与未来：让“想象”更贴近真实物理规律</span></a></li></ol></div></div></div>


</div>
<footer class="footer dis-select"><div class="social-wrap"><a class="social" href="https://github.com/MHuiG" target="_blank" rel="external nofollow noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://static.mhuig.top/gh/cdn-x/placeholder@1.0.3/social/08a41b181ce68.svg"></a><a class="social" href="https://music.163.com/#/user/home?id=63035382" target="_blank" rel="external nofollow noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://static.mhuig.top/gh/cdn-x/placeholder@1.0.3/social/3845874.svg"></a><a class="social" href="/contact/" rel="noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://static.mhuig.top/gh/cdn-x/placeholder@1.0.3/social/a1b00e20f425d.svg"></a></div></footer>

    </aside>
    <div class="l_main">
      

      


<div class="bread-nav fs12"><div id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a><span class="sep"></span><a class="cap breadcrumb" href="/">文章</a><span class="sep"></span><a class="cap breadcrumb-link" href="/categories/%E7%9F%A5%E4%B9%8E/">知乎</a> <span class="sep"></span> <a class="cap breadcrumb-link" href="/categories/%E7%9F%A5%E4%B9%8E/%E9%87%8F%E5%AD%90%E4%BD%8D/">量子位</a></div><div id="post-meta">发布于&nbsp;<time datetime="2025-10-30T10:20:04.000Z">2025-10-30</time></div></div>

<article class="content md post">
<h1 class="article-title"><span>让机器人在“想象”中学习世界的模型来了！PI联创课题组&清华陈建宇团队联合出品</span></h1>
<div>
<blockquote data-pid="wbv3ik0w">Ctrl-World团队 投稿<br>量子位 | 公众号 QbitAI</blockquote><p data-pid="N6G4VWaR">这两天，Physical Intelligence（PI）联合创始人Chelsea Finn在上，对斯坦福课题组一项最新世界模型工作kuakua连续点赞。</p><blockquote data-pid="DfQ95zvy"><b>生成看起来不错的视频很容易，难的是构建一个真正对机器人有用的通用模型</b>——它需要紧密跟随动作，还要足够准确以避免频繁幻觉。</blockquote><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://picx.zhimg.com/v2-794e46050c9f65d918a95ae9e53032ed_r.jpg" data-original-token="v2-794e46050c9f65d918a95ae9e53032ed" data-rawheight="890" data-rawwidth="640" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-794e46050c9f65d918a95ae9e53032ed_1440w.jpg" width="640"></figure><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic3.zhimg.com/v2-9681918a582a7cd66fb7a49bb18bb6be_r.jpg" data-original-token="v2-9681918a582a7cd66fb7a49bb18bb6be" data-rawheight="779" data-rawwidth="640" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-9681918a582a7cd66fb7a49bb18bb6be_1440w.jpg" width="640"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="zvAi3KUq">这项研究，正是她在斯坦福带领的课题组与清华大学陈建宇团队联合提出的<b>可控生成世界模型Ctrl-World</b>。</p><p data-pid="I9-ljpK7">这是一个能让机器人在“想象空间”中完成任务预演、策略评估与自我迭代的突破性方案。</p><p data-pid="qPz5C3PD">核心数据显示，该模型<b>使用零真机数据</b>，大幅提升策略在某些在下游任务的指令跟随能力，成功率从38.7%提升至83.4%，平均改进幅度达44.7%。</p><p data-pid="HjhXisBf">其相关论文《CTRL-WORLD：A CONTROLLABLE GENERATIVE WORLD MODEL FOR ROBOT MANIPULATION》已发布于arXiv平台。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic1.zhimg.com/v2-fb22bd7fc0a6ef1f7bf5a25d34caf104_r.jpg" data-original-token="v2-fb22bd7fc0a6ef1f7bf5a25d34caf104" data-rawheight="156" data-rawwidth="640" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-fb22bd7fc0a6ef1f7bf5a25d34caf104_1440w.jpg" width="640"></figure><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic3.zhimg.com/v2-4ca6bc540e4df3379d055cd2d28935c0_r.jpg" data-original-token="v2-4ca6bc540e4df3379d055cd2d28935c0" data-rawheight="199" data-rawwidth="640" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-4ca6bc540e4df3379d055cd2d28935c0_1440w.jpg" width="640"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="sYuI4Wwj">注：Ctrl-World专为通用机器人策略的策略在环轨迹推演而设计。它生成联合多视角预测（包括腕部视角），通过帧级条件控制实现细粒度动作控制，并通过姿态条件记忆检索维持连贯的长时程动态。这些组件实现了：（1）在想象中进行精准的策略评估，并与真实世界轨迹推演对齐（2）通过合成轨迹实现针对性的策略改进</p><h2>研究背景：机器人训练的“真实世界困境”与世界模型的破局价值</h2><p data-pid="zbP7uyLH">当前，视觉-语言-动作（VLA）模型虽在多种操作任务与场景中展现出卓越性能，但在开放世界场景中仍面临两大核心难题，这也是团队研发CTRL-WORLD的核心动因：</p><p data-pid="y1maXrgn"><b>难题一，策略评估成本高，真实测试烧钱又低效</b>。</p><p data-pid="_OadvLQx">验证机器人策略性能需在不同场景、任务中反复试错。</p><p data-pid="OJrtCuAK">以“抓取物体”任务为例，研究者需准备大小、材质、形状各异的物体，搭配不同光照、桌面纹理的环境，让机器人重复成百上千次操作。</p><p data-pid="R-AtwuX_">不仅如此，测试中还可能出现机械臂碰撞（故障率约5%-8%）、物体损坏（损耗成本单轮测试超千元）等问题，单策略评估周期常达数天。更关键的是，抽样测试无法覆盖所有潜在场景，难以全面暴露策略缺陷。</p><p data-pid="pkN2BHLi"><b>难题二，策略迭代同样难，真实场景数据永远不够用</b>。</p><p data-pid="VUVmsx7H">即便在含95k轨迹、564个场景的DROID数据集上训练的主流模型π₀.₅，面对“抓取左上角物体”“折叠带花纹毛巾”等陌生指令或“手套、订书机”等未见过的物体时，成功率仅38.7%。</p><p data-pid="mPGuVVaa">传统改进方式依赖人类专家标注新数据，但标注速度远赶不上场景更新速度——标注100条高质量折叠毛巾轨迹需资深工程师20小时，成本超万元，且无法覆盖所有异形物体与指令变体。</p><p data-pid="YAOGM31f">开放世界尚存在棘手问题，另一边，<b>传统世界模型目前也还面临三大痛点</b>——</p><p data-pid="1a9JUDGb">为解决真实世界依赖，学界曾尝试用世界模型（即虚拟模拟器）让机器人在想象中训练。</p><p data-pid="m0RUM_FJ">但研究团队在论文《CTRL-WORLD：A CONTROLLABLE GENERATIVE WORLD MODEL FOR ROBOT MANIPULATION》中指出，现有世界模型多数方法聚焦于被动视频预测场景，无法与先进通用策略进行主动交互。</p><p data-pid="fKwXRA0-">具体来说，存在三大关键局限，阻碍其支持策略在环（policy-in-the-loop）推演：</p><ul><li data-pid="EQ2PJBjS"><b>单视角导致幻觉</b></li><li data-pid="LknxUmAj">多数模型仅模拟单一第三人称视角，导致“部分可观测性问题”——例如机械臂抓取物体时，模型看不到腕部与物体的接触状态，可能出现“物体无物理接触却瞬移到夹爪中”的幻觉；</li><li data-pid="tu8xki4g"><b>动作控制不精细</b></li><li data-pid="w5bBC-Ve">传统模型多依赖文本或初始图像条件，无法绑定高频、细微的动作信号，例如机械臂“Z轴移动6厘米”与“Z轴移动4厘米”的差异无法被准确反映，导致虚拟预演与真实动作脱节；</li><li data-pid="avy2DQ8e"><b>长时一致性差</b></li><li data-pid="LkeSxWLt">随着预测时间延长，微小误差会不断累积，导致“时序漂移”——论文实验显示，传统模型在10秒预演后，物体位置与真实物理规律的偏差，失去参考价值。</li></ul><p data-pid="OmFGG9aO">为此，清华大学陈建宇与斯坦福大学Chelsea Finn两大团队<b>联合提出CTRL-WORLD，旨在构建一个“能精准模拟、可长期稳定、与真实对齐”的机器人虚拟训练空间，让机器人通过“想象”训练</b>。</p><h2>三大创新技术，让CTRL-WORLD突破传统世界模型局限</h2><p data-pid="0ShX5Psj">Ctrl-World通过三项针对性设计，解决了传统世界模型的痛点，实现“高保真、可控制、长连贯”的虚拟预演。</p><p data-pid="4RgSrMTm">论文强调，这三大创新共同将“被动视频生成模型”转化为“可与VLA策略闭环交互的模拟器”。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://picx.zhimg.com/v2-8b02e804023b39c7810c2a933b569371_r.jpg" data-original-token="v2-8b02e804023b39c7810c2a933b569371" data-rawheight="232" data-rawwidth="640" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-8b02e804023b39c7810c2a933b569371_1440w.jpg" width="640"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="ZY6rKN47">Ctrl-World基于预训练视频扩散模型初始化，并通过以下方式适配为一个可控且时间一致的世界模型：</p><ul><li data-pid="PF11EMmd">多视角输入与联合预测</li><li data-pid="wEjjjsEv">帧级动作条件控制</li><li data-pid="oLkHbAxX">姿态条件记忆检索</li></ul><h2>第一，多视角联合预测：解决“视野盲区”，降低幻觉率</h2><p data-pid="DsZCsfLG">一般来说，以往模型靠单视图预测，存在部分观测问题与幻觉。</p><p data-pid="U9UnlcNR">而<b>Ctrl-World结合第三人称与腕部视图联合预测</b>，生成的未来轨迹精准且贴合真实情况。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic1.zhimg.com/v2-6177d851f81d95dbe4d77a59ecddf0da_r.jpg" data-original-token="v2-6177d851f81d95dbe4d77a59ecddf0da" data-rawheight="246" data-rawwidth="640" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-6177d851f81d95dbe4d77a59ecddf0da_1440w.jpg" width="640"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="7QSOFEze">传统世界模型仅模拟单一第三方视角，本质是“信息不全”。</p><p data-pid="RaE-cWlK">而CTRL-WORLD创新性地联合生成第三方全局视角+腕部第一视角：</p><ul><li data-pid="z-unl83t">第三方视角提供环境全局信息（如物体在桌面的整体布局），腕部视角捕捉接触细节（如机械爪与毛巾的摩擦、与抽屉的碰撞位置）；</li><li data-pid="HqEOHb2Q">模型通过空间Transformer将多视角图像token拼接（单帧含3个192×320图像，编码为24×40latent特征），实现跨视角空间关系对齐。</li></ul><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic3.zhimg.com/v2-942ddd6908d4410ce70630191464c112_r.jpg" data-original-token="v2-942ddd6908d4410ce70630191464c112" data-rawheight="147" data-rawwidth="640" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-942ddd6908d4410ce70630191464c112_1440w.jpg" width="640"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="WPUAWtse">论文实验验证了这一设计的价值：</p><p data-pid="YPoB1-1A">在涉及机械臂与物体接触的精细操作任务中（如抓取小型物体），腕部视角可精准捕捉夹爪与物体的接触状态（如捏合力度、接触位置），显著减少“无物理接触却完成抓取的幻觉”。</p><p data-pid="wefehfLY">定量数据显示，该设计使物体交互幻觉率降低；在多视角评估中，Ctrl-World的峰值信噪比（PSNR）达23.56，远超传统单视角模型WPE（20.33）和IRASim（21.36），结构相似性（SSIM）0.828也显著高于基线（WPE0.772、IRASim0.774），<b>证明虚拟画面与真实场景的高度契合</b>。</p><h2>第二，帧级动作控制：绑定动作与视觉因果，实现厘米级精准操控</h2><p data-pid="jcVa64Jd">要让虚拟预演“可控”，必须建立“动作-视觉”的强因果关系。</p><p data-pid="qvOSt_QW">Ctrl-World的解决方案是“帧级动作绑定”：</p><ul><li data-pid="MrRwP6J_">将机器人输出的动作序列（如关节速度）转化为笛卡尔空间中的机械臂姿态参数；</li><li data-pid="qWicLffE">通过帧级交叉注意力模块，让每一帧的视觉预测都与对应的姿态参数严格对齐——就像“分镜脚本”对应每一幕剧情，确保“动作A必然导致视觉结果B”。</li></ul><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://picx.zhimg.com/v2-a34b5a5fdc087297b4eb55f31af8d67d_r.jpg" data-original-token="v2-a34b5a5fdc087297b4eb55f31af8d67d" data-rawheight="323" data-rawwidth="640" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-a34b5a5fdc087297b4eb55f31af8d67d_1440w.jpg" width="640"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="71D7UARX">注：上图展示的是Ctrl-World的可控性及其消融实验。不同的动作序列可以在Ctrl-World中以厘米级的精度产生不同的展开结果。移除记忆会导致预测模糊（蓝色），而移除帧级姿势条件会降低控制精度（紫色）。注意力可视化（左侧）在预测(t=4)秒帧时，对具有相同姿势的(t=0)秒帧显示出强烈的注意力，说明了记忆检索的有效性。为了清晰起见，每个动作块都用自然语言表达（例如，“Z轴-6厘米”）。由于空间限制，仅可视化了中间帧的腕部视角。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic3.zhimg.com/v2-da33392d1d749d6862ad6365a8a2b430_r.jpg" data-original-token="v2-da33392d1d749d6862ad6365a8a2b430" data-rawheight="190" data-rawwidth="640" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-da33392d1d749d6862ad6365a8a2b430_1440w.jpg" width="640"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="rECIiGPF">论文中给出了直观案例：</p><p data-pid="mS9I4i0V">当机械臂执行不同的空间位移或姿态调整动作时（如沿特定轴的厘米级移动、夹爪开合），Ctrl-World能生成与动作严格对应的预演轨迹，即使是细微的动作差异（如几厘米的位移变化），也能被准确区分和模拟。</p><p data-pid="pQIeXQSO">定量ablation实验显示，若移除“帧级动作条件”，模型的PSNR会从23.56降至21.20，LPIPS（感知相似度，数值越低越好）从0.091升至0.109，<b>证明该设计是精准控制的核心</b>。</p><h2>第三，姿态条件记忆检索：给长时模拟“装稳定器”，20秒长时预演不漂移</h2><p data-pid="yrzmiere">长时预演的“时序漂移”，本质是模型“忘记历史状态”。</p><p data-pid="FF027cCD">Ctrl-World引入“姿态条件记忆检索机制”，通过两个关键步骤解决：</p><ul><li data-pid="9LhGC8kh"><b>稀疏记忆采样：</b>从历史轨迹中以固定步长（如1-2秒）采样k帧（论文中k=7），避免上下文过长导致的计算负担；</li><li data-pid="nTXqR0x5"><b>姿态锚定检索：</b>将采样帧的机械臂姿态信息嵌入视觉token，在预测新帧时，模型会自动检索“与当前姿态相似的历史帧”，以历史状态校准当前预测，避免漂移。</li></ul><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic3.zhimg.com/v2-578da789b457bde9ac3e99e74f40c080_r.jpg" data-original-token="v2-578da789b457bde9ac3e99e74f40c080" data-rawheight="215" data-rawwidth="640" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-578da789b457bde9ac3e99e74f40c080_1440w.jpg" width="640"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="Xq942VOw">注：上图展示的是Ctrl-World的一致性。由于腕部摄像头的视野在单一轨迹中会发生显著变化，利用多视角信息和记忆检索对于生成一致的腕部视角预测至关重要。绿色框中突出显示的预测是从其他摄像头视角推断出来的，而红色框中的预测则是从记忆中检索得到的。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic4.zhimg.com/v2-10ae0ca07858faea5a4e7dd4ae7cee8f_r.jpg" data-original-token="v2-10ae0ca07858faea5a4e7dd4ae7cee8f" data-rawheight="147" data-rawwidth="640" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic4.zhimg.com/v2-10ae0ca07858faea5a4e7dd4ae7cee8f_1440w.jpg" width="640"></figure><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic3.zhimg.com/v2-da33392d1d749d6862ad6365a8a2b430_r.jpg" data-original-token="v2-da33392d1d749d6862ad6365a8a2b430" data-rawheight="190" data-rawwidth="640" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-da33392d1d749d6862ad6365a8a2b430_1440w.jpg" width="640"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="xtHT43vd">论文实验显示，该机制能让Ctrl-World稳定生成20秒以上的连贯轨迹，时序一致性指标FVD（视频帧距离，数值越低越好）仅97.4，远低于WPE（156.4）和IRASim（138.1）。</p><p data-pid="BM1LAxNb">ablation实验证明，若移除记忆模块，模型的FVD会从97.4升至105.5，PSNR从23.56降至23.06，<b>验证了记忆机制对长时一致性的关键作用</b>。</p><h2>实验验证：从“虚拟评估”到“策略提升”的全流程实效</h2><p data-pid="NrtpzWxw">团队在DROID机器人平台（含Panda机械臂、1个腕部相机+2个第三方相机）上开展三轮实验测试，从生成质量、评估准确性、策略优化三个维度全面验证CTRL-WORLD的性能：</p><h2>生成质量：多指标碾压传统模型</h2><p data-pid="XC9t0mD8">在10秒长轨迹生成测试中（256个随机剪辑，15步/秒动作输入），CTRL-WORLD在核心指标上全面领先基线模型（WPE、IRASim）：</p><ul><li data-pid="7KbUnpej"><b>PSNR：</b>23.56（WPE为20.33，IRASim为21.36），虚拟画面与真实场景的像素相似度提升15%-16%；</li><li data-pid="Gya8jHF4"><b>SSIM：</b>0.828（WPE为0.772，IRASim为0.774），物体形状、位置关系的结构一致性显著增强；</li><li data-pid="oZJUmmJi"><b>LPIPS：</b>0.091（WPE为0.131，IRASim为0.117），从人类视觉感知看，虚拟与真实画面几乎难以区分；</li><li data-pid="UzEb_pr2"><b>FVD：</b>97.4（WPE为156.4，IRASim为138.1），时序连贯性提升29%-38%。</li></ul><p data-pid="Mu2BPBrp">更关键的是，<b>面对训练中未见过的相机布局（如新增顶部视角），CTRL-WORLD能零样本适配，生成连贯多视角轨迹，证明其场景泛化能力</b>。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic1.zhimg.com/v2-ce5043f89dcc98b99dcdb8d32999e454_r.jpg" data-original-token="v2-ce5043f89dcc98b99dcdb8d32999e454" data-rawheight="134" data-rawwidth="640" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-ce5043f89dcc98b99dcdb8d32999e454_1440w.jpg" width="640"></figure><p class="ztext-empty-paragraph"><br></p><h2>策略评估：虚拟打分与真实表现高度对齐</h2><p data-pid="qJZXne8L">论文结果显示：</p><p data-pid="dOs5PHZW">虚拟预演的“指令跟随率”与真实世界的相关系数达0.87（拟合公式y=0.87x-0.04）。</p><p data-pid="ey7oZRiL">虚拟“任务成功率”与真实世界的相关系数达0.81（y=0.81x-0.11）。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic3.zhimg.com/v2-3d2da30f1587219f264f74d85eb1db76_r.jpg" data-original-token="v2-3d2da30f1587219f264f74d85eb1db76" data-rawheight="195" data-rawwidth="640" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-3d2da30f1587219f264f74d85eb1db76_1440w.jpg" width="640"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="GfDQtud-">这意味着，研究者无需启动真实机器人，仅通过Ctrl-World的虚拟预演，就能准确判断策略的真实性能，<b>将策略评估周期从“周级”缩短至“小时级”</b>。</p><h2>策略优化：400条虚拟轨迹实现44.7%性能飞跃</h2><p data-pid="_RKauokD">Ctrl-World的<b>终极价值在于用虚拟数据改进真实策略</b>。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic1.zhimg.com/v2-03e95d32b23886e23228892b48487900_r.jpg" data-original-token="v2-03e95d32b23886e23228892b48487900" data-rawheight="329" data-rawwidth="640" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-03e95d32b23886e23228892b48487900_1440w.jpg" width="640"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="WVSb8vr-">团队以π₀.₅为基础策略，按以下步骤进行优化（对应论文Algorithm1）：</p><ol><li data-pid="yPbJVI52"><b>虚拟探索：</b>在Ctrl-World中，通过“指令重述”（如将“放手套进盒子”改为“拿起布料放入盒子”）和“初始状态随机重置”，生成400条陌生任务的预演轨迹；</li><li data-pid="GigjGNHJ"><b>筛选高质量数据：</b>由人类标注员筛选出25-50条“成功轨迹”（如准确折叠指定方向的毛巾、抓取异形物体）；</li><li data-pid="jufwUbwo"><b>监督微调：</b>用这些虚拟成功轨迹微调π₀.₅策略。</li></ol><p data-pid="XG94ic_o">论文给出的细分任务改进数据极具说服力：</p><ul><li data-pid="gZiUeLpz"><b>空间理解任务：</b>识别“左上角物体”、“右下角物体”等指令的成功率，从平均28.75%升至87.5%；</li><li data-pid="v8qpKFcV"><b>形状理解任务：</b>区分“大/小红块”、“大/小绿块”的成功率，从43.74%升至91.25%；</li><li data-pid="FWMqoIZh"><b>毛巾折叠</b>（指定方向）：按“左右折叠”、“右左折叠”等指令执行的成功率，从57.5%升至80%；</li><li data-pid="CyWe_pL7"><b>新物体任务</b>：抓取“手套”、“订书机”等未见过物体的成功率，从25%升至75%。</li></ul><p data-pid="L0LtiIgS">综合所有陌生场景，π₀.₅的任务成功率从38.7%飙升至83.4%，平均提升44.7%——更关键的是，整个过程未消耗任何真实物理资源，成本仅为传统专家数据方法的1/20。</p><h2>研究与未来：让“想象”更贴近真实物理规律</h2><p data-pid="Y0sariEU">尽管成果显著，团队也坦言CTRL-WORLD仍有改进空间：</p><p data-pid="LzlPOWNg"><b>首先，复杂物理场景适配不足</b>。</p><p data-pid="eUVyMxSP">在“液体倾倒”“高速碰撞”等任务中，虚拟模拟与真实物理规律的偏差，主要因模型对重力、摩擦力的建模精度不足。</p><p data-pid="uSNCZ37e"><b>其次，初始观测敏感性高</b>。</p><p data-pid="5oHl5nrP">若第一帧画面模糊（如光照过暗），后续推演误差会快速累积。</p><p data-pid="Hc3Kbqn7">未来，团队计划从两方面突破——</p><p data-pid="7CDS_5l7">一方面将视频生成与强化学习结合，让机器人在虚拟世界自主探索最优策略；</p><p data-pid="LBEm54hp">另一方面扩大训练数据集（当前基于DROID），加入“厨房油污环境”、“户外光照变化”等复杂场景数据，提升模型对极端环境的适配能力。</p><p data-pid="bLiju46_">总的来说，此前机器人学习依赖“真实交互-数据收集-模型训练”的循环，本质是用物理资源换性能；而CTRL-WORLD构建了“虚拟预演-评估-优化-真实部署”的新闭环，让机器人能通过“想象”高效迭代。</p><p data-pid="NWdZvx2h">该成果的价值不仅限于实验室。</p><p data-pid="-CnihiNG"><b>对工业场景而言</b>，它可降低机械臂调试成本（单条生产线调试周期从1周缩至1天）。</p><p data-pid="17CY1IHm"><b>对家庭服务机器人而言</b>，它能快速适配“操作异形水杯”“整理不规则衣物”等个性化任务。</p><p data-pid="-4VBCwjR">随着视频扩散模型对物理规律建模的进一步精准，未来的CTRL-WORLD有望成为机器人“通用训练平台”，推动人形机器人更快走向开放世界。</p><p data-pid="my1x1Rx_">论文地址：<br><a class="external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2510.10125" rel="external nofollow noopener noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/pdf/2510.1012</span><span class="invisible">5</span><span class="ellipsis"></span></a><br>GitHub链接：<br><a class="external" href="https://link.zhihu.com/?target=https%3A//github.com/Robert-gyj/Ctrl-World" rel="external nofollow noopener noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/Robert-gyj/C</span><span class="invisible">trl-World</span><span class="ellipsis"></span></a></p><p data-pid="mcqiml1s">—完—<br>@量子位 · 追踪AI技术和产品新动态<br>深有感触的朋友，欢迎赞同、关注、分享三连վ'ᴗ' ի ❤</p>
</div>

<div>
<div class="tag-plugin link dis-select"><a class="link-card plain" title="让机器人在“想象”中学习世界的模型来了！PI联创课题组&清华陈建宇团队联合出品" href="https://zhuanlan.zhihu.com/p/1967293295546856638" target="_blank" rel="external nofollow noopener noreferrer"><div class="left"><span class="title">让机器人在“想象”中学习世界的模型来了！PI联创课题组&清华陈建宇团队联合出品</span><span class="desc fs12">https://zhuanlan.zhihu.com/p/1967293295546856638</span></div><div class="right"><div class="lazy img" data-bg="https://static.mhuig.top/gh/cdn-x/placeholder@1.0.1/link/8f277b4ee0ecd.svg"></div></div></a></div>
</div>



<div class="article-footer reveal fs14"><section id="license"><div class="header"><span>许可协议</span></div><div class="body"><p>本文采用 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/me-shaon/GLWTPL/blob/master/translations/LICENSE_zh-CN">GLWT（Good Luck With That，祝你好运）公共许可证</a> 许可协议。</p>
</div></section></div>

</article>

<div class="related-wrap reveal" id="read-next"><section class="header cap theme"><span>接下来阅读</span></section><section class="body fs14"><a id="next" href="/rss/9620cacc.html">刚刚，智源悟界·Emu3.5登场，原生具备世界建模能力<span class="note">较早</span></a><div class="line"></div><a id="prev" href="/rss/97c17374.html">Octoverse: A New Developer Joins GitHub Every Second as AI Leads TypeScript to #1<span class="note">较新</span></a></section></div>


<div class="related-wrap reveal" id="related-posts">
    <section class="header">
      <div class="title cap theme">您可能感兴趣的文章</div>
    </section>
    <section class="body">
    <div class="related-posts"><a class="item" href="/rss/ca5454bb.html" title="AI百科全书SciencePedia：当马斯克Grokipedia遭遇滑铁卢，有个中国团队默默把活儿干了"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-90b8d9d2db6036666ec62b38dba81e0f_1440w.jpg"></div><span class="title">AI百科全书SciencePedia：当马斯克Grokipedia遭遇滑铁卢，有个中国团队默默把活儿干了</span></a><a class="item" href="/rss/4b347ad2.html" title="OpenAI首个GPT-5找Bug智能体：全自动读代码找漏洞写修复"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic2.zhimg.com/v2-c3d5a9680236638717542c097d62d427_1440w.jpg"></div><span class="title">OpenAI首个GPT-5找Bug智能体：全自动读代码找漏洞写修复</span></a><a class="item" href="/rss/e8e18acd.html" title="Sora连更三大新功能！一键打造IP形象，限时免注册码抢占安卓市场"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-227408c45b662a96d699e950c7d84d4a_720w.jpg?source=d16d100b"></div><span class="title">Sora连更三大新功能！一键打造IP形象，限时免注册码抢占安卓市场</span></a><a class="item" href="/rss/569a88ec.html" title="LLM能替代数据科学家了？DeepAnalyze帮你告别手动分析数据"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pica.zhimg.com/v2-094e2b1562fe527903fc3a7208100a38_720w.jpg?source=d16d100b"></div><span class="title">LLM能替代数据科学家了？DeepAnalyze帮你告别手动分析数据</span></a><a class="item" href="/rss/dbf1bcff.html" title="今年双11，聪明人都在偷偷换AI PC"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic2.zhimg.com/v2-b78ffc0c3fb6ea966acb4d9215a621f1_1440w.jpg"></div><span class="title">今年双11，聪明人都在偷偷换AI PC</span></a><a class="item" href="/rss/aaf1746b.html" title="全球首个具身智能开放平台来了！让大模型长出“身体”，像人一样自然表达交互"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-ccaf896eff5a7b7371573797c1c6f3d2_720w.jpg?source=d16d100b"></div><span class="title">全球首个具身智能开放平台来了！让大模型长出“身体”，像人一样自然表达交互</span></a><a class="item" href="/rss/40a2b8b6.html" title="大模型公司不搞浏览器搞Agent，实测找到原因了"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-2479555c5e74fbc22221845df758e515_720w.jpg?source=d16d100b"></div><span class="title">大模型公司不搞浏览器搞Agent，实测找到原因了</span></a><a class="item" href="/rss/947efcca.html" title="原神LOL齐聚的Unity开发者大会，我看到了AI游戏的未来"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic4.zhimg.com/v2-30959dea95a59bc781a6e1ba0fc72a2f_1440w.jpg"></div><span class="title">原神LOL齐聚的Unity开发者大会，我看到了AI游戏的未来</span></a><a class="item" href="/rss/4ff7a038.html" title="有人说它能做“具身智能时代的苹果”，这家公司凭什么？"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-633c5a0d56be91d42f8fc9ec0983d259_1440w.jpg"></div><span class="title">有人说它能做“具身智能时代的苹果”，这家公司凭什么？</span></a><a class="item" href="/rss/e533cfdd.html" title="「套壳」的最高境界：OpenAI揭秘Atlas浏览器架构OWL"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-b14cd0fab50911f2501d5c15b0951aad_720w.jpg?source=d16d100b"></div><span class="title">「套壳」的最高境界：OpenAI揭秘Atlas浏览器架构OWL</span></a><a class="item" href="/rss/4ce5006b.html" title="一站看尽NeurIPS 2025前沿成果，11月22日北京见！"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-27329d9e86e297061af9abb979f64a94_1440w.jpg"></div><span class="title">一站看尽NeurIPS 2025前沿成果，11月22日北京见！</span></a><a class="item" href="/rss/87c0a85e.html" title="世界模型可单GPU秒级生成了？腾讯开源FlashWorld，效果惊艳、免费体验"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pica.zhimg.com/v2-b8803d9ec302a4a8077f238d25cc4b4c_1440w.jpg"></div><span class="title">世界模型可单GPU秒级生成了？腾讯开源FlashWorld，效果惊艳、免费体验</span></a></div></section></div>





      
<footer class="page-footer reveal fs12"><hr><div class="sitemap"><div class="sitemap-group"><span class="fs14">RSS</span><a href="/">近期</a><a href="/categories/">分类</a><a href="/tags/">标签</a><a href="/archives/">归档</a></div><div class="sitemap-group"><span class="fs14">Support</span><a target="_blank" rel="noopener" href="https://blog.mhuig.top/">博客</a><a target="_blank" rel="noopener" href="https://api.mhuig.top/">API</a><a target="_blank" rel="noopener" href="https://ssl.mhuig.top/">SSL Status</a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://mhuig.instatus.com/">Status Monitors</a></div><div class="sitemap-group"><span class="fs14">社交</span><a target="_blank" rel="noopener" href="https://blog.mhuig.top/pages/friends/">友链</a><a target="_blank" rel="noopener" href="https://blog.mhuig.top/pages/about/">留言板</a></div><div class="sitemap-group"><span class="fs14">更多</span><a target="_blank" rel="noopener" href="https://mhuig.top/">关于我</a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/MHuiG">GitHub</a><a href="/contact/">Contact</a><a href="/privacy-policy/">隐私政策</a></div></div><div class="text"><p>本站由 <a target="_blank" rel="noopener" href="https://mhuig.top/">MHuiG</a> 使用 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/xaoxuu/hexo-theme-stellar">Stellar</a> 主题创建，您可以在 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/MHG-LAB/RSSBOX">GitHub</a> 找到本站源码<br>本博客所有文章除特别声明外，均采用 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/me-shaon/GLWTPL/blob/master/translations/LICENSE_zh-CN">GLWT（Good Luck With That，祝你好运）公共许可证</a> 许可协议，转载请注明出处。</p>
</div></footer>

      <div class="float-panel mobile-only blur" style="display:none">
  <button type="button" class="sidebar-toggle mobile" onclick="sidebar.toggle()">
    <svg class="icon" style="width: 1em; height: 1em;vertical-align: middle;fill: currentColor;overflow: hidden;" viewbox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15301"><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 2.3 26.8 24.6 47.5 51.6 47.6h416.5v4z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15302"/><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 1.9 27.7 23.9 49.7 51.6 51.6h416.5z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15303"/></svg>
  </button>
</div>

    </div>
  </div>
  <div class="scripts">
    <script type="text/javascript">
  stellar = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    loadCSS: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    // 从 butterfly 和 volantis 获得灵感
    loadScript: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    // https://github.com/jerryc127/hexo-theme-butterfly
    jQuery: (fn) => {
      if (typeof jQuery === 'undefined') {
        stellar.loadScript(stellar.plugins.jQuery).then(fn)
      } else {
        fn()
      }
    }
  };
  stellar.github = 'https://github.com/xaoxuu/hexo-theme-stellar/tree/1.9.0';
  stellar.config = {
    date_suffix: {
      just: '刚刚',
      min: '分钟前',
      hour: '小时前',
      day: '天前',
      month: '个月前',
    },
  };

  // required plugins (only load if needs)
  stellar.plugins = {
    jQuery: 'https://static.mhuig.top/npm/jquery@3.5.1/dist/jquery.min.js',
    sitesjs: '/js/plugins/sites.js',
    friendsjs: '/js/plugins/friends.js',
  };

  // optional plugins
  if ('true' == 'true') {
    stellar.plugins.lazyload = Object.assign({"enable":true,"js":"https://static.mhuig.top/npm/vanilla-lazyload@17.3.1/dist/lazyload.min.js","transition":"blur"});
  }
  if ('true' == 'true') {
    stellar.plugins.swiper = Object.assign({"enable":true,"css":"https://unpkg.com/swiper@6/swiper-bundle.min.css","js":"https://unpkg.com/swiper@6/swiper-bundle.min.js"});
  }
  if ('' == 'true') {
    stellar.plugins.scrollreveal = Object.assign({"enable":null,"js":"https://static.mhuig.top/npm/scrollreveal@4.0.9/dist/scrollreveal.min.js","distance":"8px","duration":500,"interval":100,"scale":1});
  }
  if ('true' == 'true') {
    stellar.plugins.preload = Object.assign({"enable":true,"service":"flying_pages","instant_page":"https://static.mhuig.top/gh/volantis-x/cdn-volantis@4.1.2/js/instant_page.js","flying_pages":"https://static.mhuig.top/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"});
  }
  if ('true' == 'true') {
    stellar.plugins.fancybox = Object.assign({"enable":true,"js":"https://static.mhuig.top/npm/@fancyapps/ui@4.0/dist/fancybox.umd.js","css":"https://static.mhuig.top/npm/@fancyapps/ui@4.0/dist/fancybox.css","selector":".swiper-slide img,article.content img"});
  }
  if ('false' == 'true') {
    stellar.plugins.heti = Object.assign({"enable":false,"css":"https://unpkg.com/heti/umd/heti.min.css","js":"https://unpkg.com/heti/umd/heti-addon.min.js"});
  }
</script>

<!-- required -->

  
<script src="/js/main.js" async></script>



<!-- optional -->



<!-- inject -->


  </div>
</body>
</html>
