<!DOCTYPE html>
<html lang="zh-CN">

<head>
  <meta name="generator" content="Hexo 6.3.0">
  <meta charset="utf-8">
  

  <meta http-equiv="x-dns-prefetch-control" content="on">
  <link rel="dns-prefetch" href="https://fastly.jsdelivr.net">
  <link rel="preconnect" href="https://fastly.jsdelivr.net" crossorigin>
  <link rel="dns-prefetch" href="//unpkg.com">

  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" content="#f8f8f8">
  <title>刚刚，Kimi开源新架构，开始押注线性注意力 - RSSBOX</title>

  
    <meta name="description" content="机器之心报道编辑：张倩、+0Kimi 押注线性注意力，MiniMax 青睐全注意力，究竟哪条路能走得更远？在智能体时代，推理的计算需求正成为一个核心瓶颈，尤其是在长时程和强化学习场景中。此时，标准注意力机制中存在的低效问题变得更加突出。线性注意力为降低计算复杂度提供了一种有前景的方法，但由于表达能力有限，它在语言建模方面的表现历来不如 softmax 注意力，即使对于短序列也是如此。最近的进展显">
<meta property="og:type" content="article">
<meta property="og:title" content="刚刚，Kimi开源新架构，开始押注线性注意力">
<meta property="og:url" content="https://rssbox.mhuig.top/rss/5d56ae86.html">
<meta property="og:site_name" content="RSSBOX">
<meta property="og:description" content="机器之心报道编辑：张倩、+0Kimi 押注线性注意力，MiniMax 青睐全注意力，究竟哪条路能走得更远？在智能体时代，推理的计算需求正成为一个核心瓶颈，尤其是在长时程和强化学习场景中。此时，标准注意力机制中存在的低效问题变得更加突出。线性注意力为降低计算复杂度提供了一种有前景的方法，但由于表达能力有限，它在语言建模方面的表现历来不如 softmax 注意力，即使对于短序列也是如此。最近的进展显">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-363b9748f942d91888a414186797979a_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-14c4f48eb559db40f80027673501be2e_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic4.zhimg.com/v2-35f567e80ae40fcb9fcfd7fb42fe7f39_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pica.zhimg.com/v2-de7dfb43d9dd5288627cf6589238b11a_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-1c0a745540569a17caf815cb9ccc9731_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic2.zhimg.com/v2-6bf7e9ab06928853a787488719e42229_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-70a3c33abe438a4968b4c33177c2f8c7_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic2.zhimg.com/v2-f016ef997190c7e358466eb2725ee843_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-1af87a5af13d6529db9de898ea70fdbe_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-ea29d718da8404cceaf1850bdb1cab66_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-43110d528897722f710a0a4f51fbcd96_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-3e31d12241e21db3a0dddb117110e08d_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic4.zhimg.com/v2-fcf33cae06285b6d7f3d748c2dc7ce93_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic2.zhimg.com/v2-65dab0fd67439ce4ec3135c533bd3771_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-c67dffa7d1d348e8744dfc6fa713d18d_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic4.zhimg.com/v2-9c495ce5c190aebfdc5366b515b2429d_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic4.zhimg.com/v2-cafe262c21912db0a8022f20b285d117_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pica.zhimg.com/v2-9c7de8207838e8e31fd21c07943595a2_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-d355b0fa95dea13cc91d120c56c427f1_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-87ee827aed77ac807e5f8eb03a20249e_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-ad0a322621aec21e3b6d5350096f2c94_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-07e5bbd35d16e031b64e13f81fdaab14_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-7d9d8fe8ab5aff29c11850df3b947b58_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-11844e39d2693f156087eb4278362f69_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic4.zhimg.com/v2-deb4e32703de7318693102e0f1981549_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-4196c6e1a859917ec423636cde16511a_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic2.zhimg.com/v2-fabf6bc62aabdb63631e662723468647_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic2.zhimg.com/v2-cd4aad128468f8d21553a4040b74d75b_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-ef0710565c2d395fd772a92666ab67c0_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-61c6452db07fdfc1019cd8aaf95cb07c_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-510ea16a752f2db817364b9ff2bdaad8_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-73f0ef781571bbe53aa836ef68a4b01a_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-cffd25080dcfe207ab88ae87ea0b487b_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pica.zhimg.com/v2-32de24ab15aed55e15ce69703f4f6592_1440w.jpg">
<meta property="article:published_time" content="2025-10-31T06:02:58.000Z">
<meta property="article:modified_time" content="2025-10-31T06:02:58.000Z">
<meta property="article:author" content="MHuiG">
<meta property="article:tag" content="知乎">
<meta property="article:tag" content="机器之心">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-363b9748f942d91888a414186797979a_1440w.jpg">
  
  

  <!-- feed -->
  
    <link rel="alternate" href="/atom.xml" title="RSSBOX" type="application/atom+xml">
  

  
    
<link rel="stylesheet" href="/css/main.css">

  

  

  

  


  
</head>

<body>
  




  <div class="l_body" id="start">
    <aside class="l_left" layout="post">
    


<header class="header">

<div class="logo-wrap"><a class="avatar" target="_blank" rel="noopener" href="https://mhuig.top/"><div class="bg" style="opacity:0;background-image:url(https://static.mhuig.top/gh/cdn-x/placeholder@1.0.2/avatar/round/rainbow64@3x.webp);"></div><img no-lazy class="avatar" src="/avatar.png" onerror="javascript:this.classList.add('error');this.src='https://static.mhuig.top/gh/cdn-x/placeholder@1.0.1/image/2659360.svg';"></a><a class="title" href="/"><div class="main">RSSBOX</div><div class="sub normal cap">MHuiGのRSS订阅</div><div class="sub hover cap" style="opacity:0">rssbox.mhuig.top</div></a></div>
<nav class="menu dis-select"><a class="nav-item active" href="/">RSS</a><a class="nav-item" target="_blank" rel="noopener" href="https://blog.mhuig.top/">博客</a><a class="nav-item" target="_blank" rel="noopener" href="https://mhuig.top/">关于</a></nav></header>

<div class="widgets">


<div class="widget-wrap" id="recent"><div class="widget-header cap dis-select"><span class="name">最近更新</span><a class="cap-action" id="rss" title="Subscribe" href="/atom.xml"><svg class="icon" viewbox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="8938"><path d="M800.966 947.251c0-404.522-320.872-732.448-716.69-732.448V62.785c477.972 0 865.44 395.987 865.44 884.466h-148.75z m-162.273 0h-148.74c0-228.98-181.628-414.598-405.678-414.598v-152.01c306.205 0 554.418 253.68 554.418 566.608z m-446.24-221.12c59.748 0 108.189 49.503 108.189 110.557 0 61.063-48.44 110.563-108.188 110.563-59.747 0-108.18-49.5-108.18-110.563 0-61.054 48.433-110.556 108.18-110.556z" p-id="8939"/></svg></a></div><div class="widget-body fs14"><div class="more-item"><a class="title" href="/rss/ee09a392.html">安德尔施帕赫-特普利采岩石林的哥特式拱门, 捷克 (© Kseniya_Milner/Getty Images)</a></div><div class="more-item"><a class="title" href="/rss/ba5ac1e6.html">岚山缤纷的枫叶与竹林, 京都, 日本 (© DoctorEgg/Getty Images)</a></div><div class="more-item"><a class="title" href="/rss/3015d56f.html">布兰城堡入口, 布拉索夫, 罗马尼亚 (© Blue Sky in My pocket/Getty Images)</a></div><div class="more-item"><a class="title" href="/rss/b11a76e.html">杰伊瑟尔梅尔的骆驼, 拉贾斯坦邦, 印度 (© f9photos/Getty Images)</a></div><div class="more-item"><a class="title" href="/rss/1287d9b2.html">法纳尔森林里的古老月桂树，马德拉群岛，葡萄牙 (© Lukas Jonaitis/Shutterstock)</a></div></div></div>



</div>
<footer class="footer dis-select"><div class="social-wrap"><a class="social" href="https://github.com/MHuiG" target="_blank" rel="external nofollow noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://static.mhuig.top/gh/cdn-x/placeholder@1.0.3/social/08a41b181ce68.svg"></a><a class="social" href="https://music.163.com/#/user/home?id=63035382" target="_blank" rel="external nofollow noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://static.mhuig.top/gh/cdn-x/placeholder@1.0.3/social/3845874.svg"></a><a class="social" href="/contact/" rel="noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://static.mhuig.top/gh/cdn-x/placeholder@1.0.3/social/a1b00e20f425d.svg"></a></div></footer>

    </aside>
    <div class="l_main">
      

      


<div class="bread-nav fs12"><div id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a><span class="sep"></span><a class="cap breadcrumb" href="/">文章</a><span class="sep"></span><a class="cap breadcrumb-link" href="/categories/%E7%9F%A5%E4%B9%8E/">知乎</a> <span class="sep"></span> <a class="cap breadcrumb-link" href="/categories/%E7%9F%A5%E4%B9%8E/%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83/">机器之心</a></div><div id="post-meta">发布于&nbsp;<time datetime="2025-10-31T06:02:58.000Z">2025-10-31</time></div></div>

<article class="content md post">
<h1 class="article-title"><span>刚刚，Kimi开源新架构，开始押注线性注意力</span></h1>
<div>
<p data-pid="t76fQwMu">机器之心报道</p><p data-pid="bJV1HQ93"><b>编辑：张倩、+0</b></p><blockquote data-pid="rgiLvBdj">Kimi 押注线性注意力，MiniMax 青睐全注意力，究竟哪条路能走得更远？</blockquote><p data-pid="V5OuFSut">在智能体时代，推理的计算需求正成为一个核心瓶颈，尤其是在长时程和强化学习场景中。此时，标准注意力机制中存在的低效问题变得更加突出。</p><p data-pid="Poz7c90a">线性注意力为降低计算复杂度提供了一种有前景的方法，但由于表达能力有限，它在语言建模方面的表现历来不如 softmax 注意力，即使对于短序列也是如此。</p><p data-pid="hIgomjzA">最近的进展显著缩小了这一差距，主要得益于两项创新：门控或衰减机制以及 delta 规则。这些进展共同推动线性注意力在中等长度序列上的性能接近 softmax 水平。尽管如此，纯粹的线性结构从根本上仍受限于有限状态容量，这使得长序列建模和上下文内检索在理论上仍具有挑战性。</p><p data-pid="5d8IlY66">因此，结合 softmax 注意力和线性注意力的混合架构成为在质量和效率之间的一种折衷方案。但之前的混合模型往往规模较小，缺乏多样化基准评估。关键挑战是开发出一种新的注意力架构，能够在速度和内存上显著提高效率，同时保证或超过全注意力的质量，推动下一代解码密集型 LLM 的发展。</p><p data-pid="-Shsg8sf">最近，月之暗面在这一方向有所突破。在一篇新的技术报告中，他们提出了一种新的混合线性注意力架构 ——Kimi Linear。该架构在各种场景中都优于传统的全注意力方法，包括短文本、长文本以及强化学习的 scaling 机制。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic1.zhimg.com/v2-363b9748f942d91888a414186797979a_r.jpg" data-original-token="v2-363b9748f942d91888a414186797979a" data-rawheight="353" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-363b9748f942d91888a414186797979a_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><ul><li data-pid="Jbdefaqi">技术报告：KIMI LINEAR: AN EXPRESSIVE, EFFICIENT ATTENTION ARCHITECTURE</li><li data-pid="_9fTv3qm">报告链接：<a class="external" href="https://link.zhihu.com/?target=https%3A//github.com/MoonshotAI/Kimi-Linear/blob/master/tech_report.pdf" rel="external nofollow noopener noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/MoonshotAI/K</span><span class="invisible">imi-Linear/blob/master/tech_report.pdf</span><span class="ellipsis"></span></a></li><li data-pid="LzACp8uM">GitHub 链接：<a class="external" href="https://link.zhihu.com/?target=https%3A//github.com/MoonshotAI/Kimi-Linear%3Ftab%3Dreadme-ov-file" rel="external nofollow noopener noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/MoonshotAI/K</span><span class="invisible">imi-Linear?tab=readme-ov-file</span><span class="ellipsis"></span></a></li><li data-pid="6gZamFRs">HuggingFace 链接：<a class="external" href="https://link.zhihu.com/?target=https%3A//huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct" rel="external nofollow noopener noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">huggingface.co/moonshot</span><span class="invisible">ai/Kimi-Linear-48B-A3B-Instruct</span><span class="ellipsis"></span></a></li></ul><p data-pid="a6YEG_Q-">Kimi Linear 的核心是 Kimi Delta 注意力（KDA）—— 这是 Gated DeltaNet（GDN）的改进版本，引入了更高效的门控机制，以优化有限状态 RNN 内存的使用。作者表示，虽然 GDN 与 Mamba2 类似，采用了粗糙的 head-wise 遗忘门，但 KDA 引入了一种 channel-wise 的变体，其中每个特征维度都保持独立的遗忘率，类似于门控线性注意力（GLA）。</p><p data-pid="YhF-qkHW">这种细粒度的设计能够更精确地调控有限状态 RNN 的记忆，从而在混合架构中释放 RNN 风格模型的潜力。关键在于，KDA 通过 Diagonal-Plus-Low-Rank（DPLR）矩阵的一种专门变体来参数化其转换动态，这使得一种定制的分块并行算法成为可能，该算法相较于通用的 DPLR 公式能显著减少计算量，同时仍与经典的 delta 规则保持一致。Kimi Linear 以 3:1 的固定比例将 KDA 与周期性的全注意力层交错排列。</p><p data-pid="20-_RPrR">作者基于 KDA 与多头潜在注意力（MLA）的分层混合架构，预训练了一个 Kimi Linear 模型。该模型激活参数为 3B，总参数达 48B。</p><p data-pid="uiN0BLpj">Kimi Linear 实现了卓越的性能和硬件效率，尤其在长上下文任务中表现突出。它最多可将对大型 KV 缓存的需求减少 75%，并且在处理长达 100 万个 token 的上下文时，能将解码吞吐量提升到完整 MLA 模型的 6 倍。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic1.zhimg.com/v2-14c4f48eb559db40f80027673501be2e_r.jpg" data-original-token="v2-14c4f48eb559db40f80027673501be2e" data-rawheight="531" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-14c4f48eb559db40f80027673501be2e_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic4.zhimg.com/v2-35f567e80ae40fcb9fcfd7fb42fe7f39_r.jpg" data-original-token="v2-35f567e80ae40fcb9fcfd7fb42fe7f39" data-rawheight="425" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic4.zhimg.com/v2-35f567e80ae40fcb9fcfd7fb42fe7f39_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="iOXNmMpc">月之暗面团队在 FLA 中开源了 KDA 内核，并发布了两个版本的模型检查点，这些检查点是用 5.7 万亿个 token 训练的。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pica.zhimg.com/v2-de7dfb43d9dd5288627cf6589238b11a_r.jpg" data-original-token="v2-de7dfb43d9dd5288627cf6589238b11a" data-rawheight="265" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pica.zhimg.com/v2-de7dfb43d9dd5288627cf6589238b11a_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="GC7qftmz">整个项目的核心看点包括：</p><ul><li data-pid="riM7eku8">Kimi Delta 注意力（KDA）：一种线性注意力机制，通过细粒度门控优化门控 Delta 规则。</li><li data-pid="i4HpQeug">混合架构：3:1 的 KDA 与全局 MLA 比例在降低内存使用的同时，保持甚至超越全注意力的质量。</li><li data-pid="H2fYjVu3">卓越性能：在多种任务中表现优于全注意力，包括在 1.4 万亿 token 的训练运行中，通过公平对比，在长上下文和强化学习风格的基准测试中均有出色表现。</li><li data-pid="vl0GEWrW">高吞吐量：实现了高达 6 倍的更快解码速度，并显著减少了每个输出 token 的时间（TPOT）。</li></ul><p data-pid="fohbZpYb">目前，vLLM 已经官宣支持 Kimi Linear。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://picx.zhimg.com/v2-1c0a745540569a17caf815cb9ccc9731_r.jpg" data-original-token="v2-1c0a745540569a17caf815cb9ccc9731" data-rawheight="1115" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-1c0a745540569a17caf815cb9ccc9731_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="lyossLnV">在技术报告发布后，Kimi 的研究员「熊狸」在 X 上表示：「我很荣幸在过去一年中见证了这项伟大的工作。线性注意力在表达能力上具有巨大潜力，但在处理长上下文时存在较高的风险。KDA 不是一个玩具式的想法，它经受住了各种不可作弊的内部评估，这些评估的目的就是为了验证其有效性。」</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic2.zhimg.com/v2-6bf7e9ab06928853a787488719e42229_r.jpg" data-original-token="v2-6bf7e9ab06928853a787488719e42229" data-rawheight="728" data-rawwidth="934" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic2.zhimg.com/v2-6bf7e9ab06928853a787488719e42229_1440w.jpg" width="934"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="JRy7vuzp">该项目的重要贡献者 Zongyu Lin（目前在 UCLA）表示，「坦白说，这只是一个中间阶段，最终我们仍然在朝着实现无限上下文模型迈进。只要我们使用全局注意力，长时间解码依然受到其限制，而线性注意力背后仍然有一些基础设施挑战。但我相信这些问题都会被解决，而且来自不同实验室 / 公司的更多令人激动的成果即将到来。」</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://picx.zhimg.com/v2-70a3c33abe438a4968b4c33177c2f8c7_r.jpg" data-original-token="v2-70a3c33abe438a4968b4c33177c2f8c7" data-rawheight="1574" data-rawwidth="952" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-70a3c33abe438a4968b4c33177c2f8c7_1440w.jpg" width="952"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="CNvabad2">所以，K3 也会延续线性注意力架构吗？</p><p data-pid="NxbKPnk0">而另外的大模型玩家中，Qwen 也曾表示要大胆押注线性注意力：</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic2.zhimg.com/v2-f016ef997190c7e358466eb2725ee843_r.jpg" data-original-token="v2-f016ef997190c7e358466eb2725ee843" data-rawheight="832" data-rawwidth="946" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic2.zhimg.com/v2-f016ef997190c7e358466eb2725ee843_1440w.jpg" width="946"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="tEu6O5Rs">但 MiniMax 似乎更青睐全注意力。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic3.zhimg.com/v2-1af87a5af13d6529db9de898ea70fdbe_r.jpg" data-original-token="v2-1af87a5af13d6529db9de898ea70fdbe" data-rawheight="1259" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-1af87a5af13d6529db9de898ea70fdbe_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="Opoznwo0">到底哪个方向能走得更远？我们拭目以待。</p><p data-pid="bxk03Q3-">以下是 Kimi Linear 的技术细节。</p><p data-pid="yeX1OhLO"><b>Kimi Delta Attention：通过细粒度门控改进 Delta 规则</b></p><p data-pid="0HlztOko">Kimi Delta Attention (KDA) 是一种新型的门控线性注意力变体。它通过引入一个细粒度的对角化门控 来改进 GDN 的标量衰减，从而实现了对记忆衰减和位置感知的细粒度控制。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="content_image lazy" data-caption data-original-token="v2-ea29d718da8404cceaf1850bdb1cab66" data-rawheight="42" data-rawwidth="134" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-ea29d718da8404cceaf1850bdb1cab66_1440w.jpg" width="134"></figure><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic1.zhimg.com/v2-43110d528897722f710a0a4f51fbcd96_r.jpg" data-original-token="v2-43110d528897722f710a0a4f51fbcd96" data-rawheight="228" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-43110d528897722f710a0a4f51fbcd96_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="XYv2ulIT"><b>硬件高效的分块算法</b></p><p data-pid="7P49YnQH">通过将公式 1 的递归部分展开为分块公式，可得到：</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://picx.zhimg.com/v2-3e31d12241e21db3a0dddb117110e08d_r.jpg" data-original-token="v2-3e31d12241e21db3a0dddb117110e08d" data-rawheight="146" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-3e31d12241e21db3a0dddb117110e08d_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><ul><li data-pid="rhvcstY7">WY Representation</li></ul><p data-pid="ZmGt0U0U">通常用于将一系列秩 - 1 更新打包成单个紧凑的表示。该方法遵循了 Comba 中 P 的公式，以减少后续计算中额外矩阵求逆的需求。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic4.zhimg.com/v2-fcf33cae06285b6d7f3d748c2dc7ce93_r.jpg" data-original-token="v2-fcf33cae06285b6d7f3d748c2dc7ce93" data-rawheight="105" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic4.zhimg.com/v2-fcf33cae06285b6d7f3d748c2dc7ce93_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="P9_vwM-0">其中，辅助向量 和 通过以下递归关系计算得出：</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="content_image lazy" data-caption data-original-token="v2-65dab0fd67439ce4ec3135c533bd3771" data-rawheight="41" data-rawwidth="110" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic2.zhimg.com/v2-65dab0fd67439ce4ec3135c533bd3771_1440w.jpg" width="110"></figure><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="content_image lazy" data-caption data-original-token="v2-c67dffa7d1d348e8744dfc6fa713d18d" data-rawheight="36" data-rawwidth="117" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-c67dffa7d1d348e8744dfc6fa713d18d_1440w.jpg" width="117"></figure><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic4.zhimg.com/v2-9c495ce5c190aebfdc5366b515b2429d_r.jpg" data-original-token="v2-9c495ce5c190aebfdc5366b515b2429d" data-rawheight="294" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic4.zhimg.com/v2-9c495ce5c190aebfdc5366b515b2429d_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><ul><li data-pid="Xhys5LUN">UT transform</li></ul><p data-pid="8ouAh9di">该算法应用了 UT transform 来减少非矩阵乘法的 FLOPs，这对于在训练期间提升硬件利用率至关重要。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic4.zhimg.com/v2-cafe262c21912db0a8022f20b285d117_r.jpg" data-original-token="v2-cafe262c21912db0a8022f20b285d117" data-rawheight="197" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic4.zhimg.com/v2-cafe262c21912db0a8022f20b285d117_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="TJ8x2svr">下三角矩阵的逆可以通过高斯消元法中的前向替换，采用逐行迭代的方法高效计算。等效地，以矩阵形式，可以按如下方式分块更新状态：</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pica.zhimg.com/v2-9c7de8207838e8e31fd21c07943595a2_r.jpg" data-original-token="v2-9c7de8207838e8e31fd21c07943595a2" data-rawheight="94" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pica.zhimg.com/v2-9c7de8207838e8e31fd21c07943595a2_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://picx.zhimg.com/v2-d355b0fa95dea13cc91d120c56c427f1_r.jpg" data-original-token="v2-d355b0fa95dea13cc91d120c56c427f1" data-rawheight="172" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-d355b0fa95dea13cc91d120c56c427f1_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="WS5G1tbV">在输出阶段，该算法采用块间递归和块内并行的策略来最大化矩阵乘法吞吐量，从而充分利用 Tensor Cores 的计算潜力。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic3.zhimg.com/v2-87ee827aed77ac807e5f8eb03a20249e_r.jpg" data-original-token="v2-87ee827aed77ac807e5f8eb03a20249e" data-rawheight="268" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-87ee827aed77ac807e5f8eb03a20249e_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="VNoJFom2"><b>效率分析</b></p><p data-pid="NHgMErmW">在表示能力方面，KDA 与广义的 DPLR 公式一致，即</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic1.zhimg.com/v2-ad0a322621aec21e3b6d5350096f2c94_r.jpg" data-original-token="v2-ad0a322621aec21e3b6d5350096f2c94" data-rawheight="103" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-ad0a322621aec21e3b6d5350096f2c94_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="AmZbyizo">两者都表现出细粒度的衰减行为。然而，这种细粒度的衰减会在除法运算（例如，公式 9 中的块内计算）过程中引入数值精度问题。</p><p data-pid="oSIWIi4F">为了解决这个问题，以往的工作（如 GLA）在对数域中执行计算，并在全精度下引入二级分块。然而，这种方法妨碍了半精度矩阵乘法的充分利用，并显著降低了算子速度。</p><p data-pid="BjfkbdBb">通过将变量 a 和 b 同时绑定到 k，KDA 有效地缓解了这一瓶颈 —— 将二级分块矩阵计算的数量从四次减少到两次，并进一步消除了三次额外的矩阵乘法。因此，与 DPLR 公式相比，KDA 的算子效率提升了大约 100%。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic1.zhimg.com/v2-07e5bbd35d16e031b64e13f81fdaab14_r.jpg" data-original-token="v2-07e5bbd35d16e031b64e13f81fdaab14" data-rawheight="603" data-rawwidth="835" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-07e5bbd35d16e031b64e13f81fdaab14_1440w.jpg" width="835"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="Fw-7bLa6">在批量大小统一为 1 且头数为 16 的条件下，算子随输入长度变化的执行时间。</p><p data-pid="-3tH0Sji"><b>Kimi 线性模型架构</b></p><p data-pid="rCB7QHRS">该模型架构的主干遵循 Moonlight 的设计。除了细粒度门控之外，该模型还利用了几个组件来进一步提高 Kimi Linear 的表达能力。Kimi Linear 的整体架构如图 3 所示。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic3.zhimg.com/v2-7d9d8fe8ab5aff29c11850df3b947b58_r.jpg" data-original-token="v2-7d9d8fe8ab5aff29c11850df3b947b58" data-rawheight="932" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-7d9d8fe8ab5aff29c11850df3b947b58_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="_fX7Bchd"><b>神经参数化</b></p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://picx.zhimg.com/v2-11844e39d2693f156087eb4278362f69_r.jpg" data-original-token="v2-11844e39d2693f156087eb4278362f69" data-rawheight="1082" data-rawwidth="1104" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-11844e39d2693f156087eb4278362f69_1440w.jpg" width="1104"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="aqyAvZpD">在这里，输出门采用了类似于遗忘门的低秩参数化，以确保公平的参数比较，同时保持与全秩门控相当的性能，并缓解注意力下沉问题。</p><p data-pid="I_viaGir"><b>混合模型架构</b></p><p data-pid="FgKLIKqC">长上下文检索仍然是纯线性注意力的主要瓶颈，因此，本文将 KDA 与少数几个全全局注意力 (Full MLA) 层进行了混合。</p><p data-pid="g_MVP5zy">对于 Kimi Linear，研究人员选择了一种逐层方法（交替使用整个层），而不是逐头方法（在层内混合不同的头），因为前者在基础设施简单性和训练稳定性方面更具优势。</p><p data-pid="oiCc1FC0">经验表明，采用统一的 3:1 比例，即每 3 个 KDA 层重复 1 个全 MLA 层，可提供最佳的质量 - 吞吐量权衡。</p><p data-pid="sObfgska"><b>MLA 层的无位置编码 (NoPE)</b></p><p data-pid="ct_SGp_b">在 Kimi Linear 中，NoPE 被应用于所有全注意力层。这种设计将编码位置信息和近期偏好的全部责任委托给了 KDA 层。</p><p data-pid="TLMZJNgn">因此，KDA 被确立为主要的位置感知算子，其扮演的角色类似于（甚至可以说强于）短卷积或 SWA 等辅助组件。这一发现与先前的研究结果一致，后者同样证明了：使用一个专用的位置感知机制来补充全局 NoPE 注意力，可以产生具有竞争力的长上下文性能。</p><p data-pid="awBrKQD_">值得注意的是，NoPE 提供了显著的实用优势，特别是对于 MLA 层。首先，NoPE 使得这些层在推理过程中可以转换为高效的纯多查询注意力。其次，它简化了长上下文训练，因为它避免了 RoPE（旋转位置编码）的参数调整需求，例如频率基调优或使用像 YaRN 这样的方法。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic4.zhimg.com/v2-deb4e32703de7318693102e0f1981549_r.jpg" data-original-token="v2-deb4e32703de7318693102e0f1981549" data-rawheight="586" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic4.zhimg.com/v2-deb4e32703de7318693102e0f1981549_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="JsABp_Px"><b>实验结果</b></p><p data-pid="KeR604S4"><b>对 Kimi Linear 关键组件的消融实验</b></p><p data-pid="FS0kG7o9">作者通过将不同模型与 first-scale scaling law 模型（即 16 个注意力头、16 层）进行直接比较，开展了一系列消融研究。下表 1 记录了训练和验证的困惑度（PPL）。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic3.zhimg.com/v2-4196c6e1a859917ec423636cde16511a_r.jpg" data-original-token="v2-4196c6e1a859917ec423636cde16511a" data-rawheight="331" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-4196c6e1a859917ec423636cde16511a_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="-POpnWtq">从表中还可以得出以下信息：</p><ul><li data-pid="A70QivCB">输出门控：移除门控会降低性能，swish 门控性能明显不如 Sigmoid。</li><li data-pid="aKoT2WOL">卷积层：卷积层在混合模型中仍然发挥着不可忽视的作用。</li><li data-pid="6Tc5MRQy">混合比例：在测试的配置中，3:1 的比例（每 1 个 MLA 层对应 3 个 KDA 层）产生了最佳结果，实现了最低的训练损失和验证损失。</li></ul><p data-pid="GeUqNOah">NoPE vs. RoPE 的对比结果见表 5。从中可以看出，采用 NoPE 的 Kimi Linear 在不同的长上下文基准测试中取得了最佳的平均分数。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic2.zhimg.com/v2-fabf6bc62aabdb63631e662723468647_r.jpg" data-original-token="v2-fabf6bc62aabdb63631e662723468647" data-rawheight="243" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic2.zhimg.com/v2-fabf6bc62aabdb63631e662723468647_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="MAn6k2Le"><b>Kimi Linear 的 scaling law</b></p><p data-pid="KM0jeUMD">作者基于 Moonlight 架构，对一系列 MoE 模型进行了 scaling law 实验。在所有实验中，他们从 64 个专家中激活了 8 个，并使用了 Muon 优化器。详细信息和超参数列于表 2 中。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic2.zhimg.com/v2-cd4aad128468f8d21553a4040b74d75b_r.jpg" data-original-token="v2-cd4aad128468f8d21553a4040b74d75b" data-rawheight="368" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic2.zhimg.com/v2-cd4aad128468f8d21553a4040b74d75b_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="_4hOoss9">对于 MLA，他们遵循 Chinchilla scaling law，训练了五个不同规模的语言模型，并通过网格搜索仔细调整它们的超参数，以确保每个模型都能达到最佳性能。对于 KDA，他们保持了表 1 中验证的 3:1 这一最佳混合比例。除此之外，他们严格遵循 MLA 的训练配置，未做任何修改。如图 5 所示，与经过计算优化训练的 MLA 基线相比，Kimi Linear 的计算效率约为其 1.16 倍。作者预计，仔细的超参数调整将为 KDA 带来更优的缩放曲线。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic1.zhimg.com/v2-ef0710565c2d395fd772a92666ab67c0_r.jpg" data-original-token="v2-ef0710565c2d395fd772a92666ab67c0" data-rawheight="776" data-rawwidth="904" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-ef0710565c2d395fd772a92666ab67c0_1440w.jpg" width="904"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="aSF4k1gm"><b>与基线对比的主要实验结果</b></p><p data-pid="YqWhq3Dt">Kimi Linear @1.4T 结果：</p><ul><li data-pid="ybDPaWB1">预训练结果</li></ul><p data-pid="HFk5IMnh">在表 3 中，团队使用一个 1.4T 的预训练语料库，比较了 Kimi Linear 模型与两个基线模型（MLA 和 hybrid GDN-H）。评估重点关注三个领域：通用知识、推理（数学和代码）以及中文任务。Kimi Linear 在几乎所有类别中都持续优于两个基线模型。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic1.zhimg.com/v2-61c6452db07fdfc1019cd8aaf95cb07c_r.jpg" data-original-token="v2-61c6452db07fdfc1019cd8aaf95cb07c" data-rawheight="700" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-61c6452db07fdfc1019cd8aaf95cb07c_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><ul><li data-pid="KhjDujY9"><b>通用知识</b>： Kimi Linear 在所有关键基准（如 BBH, MMLU 和 HellaSwag）上均获得最高分。</li><li data-pid="pb2IP8d4"><b>推理</b>： 它在数学（GSM8K）和大多数代码任务（CRUXEval）上处于领先地位。然而，在 EvalPlus 上的得分略低于 GDN-H。</li><li data-pid="OUW7O6NI"><b>中文任务</b>： Kimi Linear 在 CEval 和 CMMLU 上取得了最高分。</li></ul><p data-pid="8yAVPdOr">总之，Kimi Linear 展现了最强的性能，使其成为短上下文预训练中全注意力架构的有力替代方案。</p><ul><li data-pid="gofuDtPH">SFT 结果</li></ul><p data-pid="A7VuBMwK">在经历了相同的监督微调（SFT）流程后，Kimi Linear 在通用任务以及数学和代码任务上均表现出强劲性能，持续优于 MLA 和 GDN-H。</p><p data-pid="CCQ68F_C">在通用任务中，Kimi Linear 全面领先，在各种 MMLU 基准、BBH 和 GPQA-Diamond 上均取得了最高分。在数学和代码任务中，它在 AIME 2025、HMMT 2025、PolyMath-en 和 LiveCodeBench 等高难度基准上超越了两个基线模型。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic1.zhimg.com/v2-510ea16a752f2db817364b9ff2bdaad8_r.jpg" data-original-token="v2-510ea16a752f2db817364b9ff2bdaad8" data-rawheight="543" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-510ea16a752f2db817364b9ff2bdaad8_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="e4prK3rl">尽管在 MATH500 和 EvalPlus 等个别项目上存在微小例外，但 Kimi Linear 在各项任务中均显示出稳健的优势，证实了其相较于其他测试模型（GDN-H 和 MLA）的明显优越性。</p><ul><li data-pid="3ndcbWi-">长上下文性能评估</li></ul><p data-pid="F1dv921Z">团队在 128k 上下文长度下，评估了 Kimi Linear 相对于三个基线模型 ——MLA、GDN-H 和 Kimi Linear (RoPE)—— 在几个基准上的长上下文性能（见表 5）。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic3.zhimg.com/v2-73f0ef781571bbe53aa836ef68a4b01a_r.jpg" data-original-token="v2-73f0ef781571bbe53aa836ef68a4b01a" data-rawheight="184" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-73f0ef781571bbe53aa836ef68a4b01a_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="3k_ang1J">结果凸显了 Kimi Linear 在这些长上下文任务中的明显优势。它持续优于 MLA 和 GDN-H，在 RULER (84.3) 和 RepoQA (68.5) 上以显著优势取得了最高分。这种超越模式在除 LongBench V2 和 Frames 之外的大多数其他任务中也得以保持。</p><p data-pid="OXIm_NOY">总体而言，Kimi Linear 取得了最高的平均分 (54.5)，进一步巩固了其作为长上下文场景中领先注意力架构的有效性。</p><ul><li data-pid="0dgOFV4I">RL 结果</li></ul><p data-pid="1T59fjO0">为了比较 Kimi Linear 和 MLA 的强化学习（RL）收敛特性，团队使用了 内部数学训练集进行了 RLVR，并在数学测试集（例如 AIME 2025, MATH500）上进行评估，同时保持算法和所有超参数一致，以确保公平的性能比较。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://picx.zhimg.com/v2-cffd25080dcfe207ab88ae87ea0b487b_r.jpg" data-original-token="v2-cffd25080dcfe207ab88ae87ea0b487b" data-rawheight="358" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-cffd25080dcfe207ab88ae87ea0b487b_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="GTacBS6h">如图 6 所示，与 MLA 相比，Kimi Linear 展示了更高的效率。在训练集上，尽管两个模型起点相似，但 Kimi Linear 的训练准确率增长速度明显高于 MLA，且差距逐渐拉大。在测试集上也观察到了类似现象。例如，在 MATH500 和 AIME2025 上，Kimi Linear 相比 MLA 取得了更快、更好的提升。</p><p data-pid="G1UEGw3N">总体而言，团队根据经验观察到，在强化学习下的推理密集型长文本生成中，Kimi Linear 的表现明显优于 MLA。</p><ul><li data-pid="MNhTN9Ds">总体研究结果总结</li></ul><p data-pid="7MU1DU2A">在预训练和 SFT 阶段，一个清晰的性能层级得以确立：Kimi Linear 优于 GDN-H，而 GDN-H 又优于 MLA。然而，这个层级在长上下文评估中发生了变化。虽然 Kimi Linear 保持了其领先地位，但 GDN-H 的性能有所下降，使其排名落后于 MLA。</p><p data-pid="Nn5DQ45G">此外，在 RL 阶段，Kimi Linear 也表现出优于 MLA 的性能。总体而言，Kimi Linear 在所有阶段始终名列前茅，确立了其作为全注意力架构的卓越替代方案的地位。</p><p data-pid="dguRThuH"><b>效率对比结果</b></p><p data-pid="UXYdzI0L">作者在图 7a 和图 7b 中比较了全注意力 MLA、GDN-H 和 Kimi Linear 的训练及解码时间。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pica.zhimg.com/v2-32de24ab15aed55e15ce69703f4f6592_r.jpg" data-original-token="v2-32de24ab15aed55e15ce69703f4f6592" data-rawheight="460" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pica.zhimg.com/v2-32de24ab15aed55e15ce69703f4f6592_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="KD7TjXBT">作者观察到，尽管融入了更精细的衰减机制，但 Kimi Linear 在预填充期间相较于 GDN-H 仅引入了可忽略不计的延迟开销。如图 7a 所示，它们的性能曲线几乎难以区分，这证实了 Kimi Linear 的方法保持了较高的效率。随着序列长度的增加，混合的 Kimi Linear 模型相较于 MLA 基线展现出明显的效率优势。虽然在较短长度（4k–16k）时其性能与 MLA 相当，但从 128k 开始，它的速度显著提升。这种效率差距在规模扩大时急剧拉大。如图 1b 所示，Kimi Linear 在解码阶段充分展现了其优势。在 1M 上下文长度下进行解码时，Kimi Linear 的速度是全注意力的 6 倍。</p><p data-pid="dCF-qY5y">更多技术细节请参见原论文。</p>
</div>

<div>
<div class="tag-plugin link dis-select"><a class="link-card plain" title="刚刚，Kimi开源新架构，开始押注线性注意力" href="https://zhuanlan.zhihu.com/p/1967592208057242310" target="_blank" rel="external nofollow noopener noreferrer"><div class="left"><span class="title">刚刚，Kimi开源新架构，开始押注线性注意力</span><span class="desc fs12">https://zhuanlan.zhihu.com/p/1967592208057242310</span></div><div class="right"><div class="lazy img" data-bg="https://static.mhuig.top/gh/cdn-x/placeholder@1.0.1/link/8f277b4ee0ecd.svg"></div></div></a></div>
</div>



<div class="article-footer reveal fs14"><section id="license"><div class="header"><span>许可协议</span></div><div class="body"><p>本文采用 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/me-shaon/GLWTPL/blob/master/translations/LICENSE_zh-CN">GLWT（Good Luck With That，祝你好运）公共许可证</a> 许可协议。</p>
</div></section></div>

</article>

<div class="related-wrap reveal" id="read-next"><section class="header cap theme"><span>接下来阅读</span></section><section class="body fs14"><a id="next" href="/rss/4b347ad2.html">OpenAI首个GPT-5找Bug智能体：全自动读代码找漏洞写修复<span class="note">较早</span></a><div class="line"></div><a id="prev" href="/rss/d620cf2a.html">万源共振，智构未来，全球开源技术峰会 GOTC 2025 在京开幕<span class="note">较新</span></a></section></div>


<div class="related-wrap reveal" id="related-posts">
    <section class="header">
      <div class="title cap theme">您可能感兴趣的文章</div>
    </section>
    <section class="body">
    <div class="related-posts"><a class="item" href="/rss/e533cfdd.html" title="「套壳」的最高境界：OpenAI揭秘Atlas浏览器架构OWL"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-b14cd0fab50911f2501d5c15b0951aad_720w.jpg?source=d16d100b"></div><span class="title">「套壳」的最高境界：OpenAI揭秘Atlas浏览器架构OWL</span></a><a class="item" href="/rss/4ce5006b.html" title="一站看尽NeurIPS 2025前沿成果，11月22日北京见！"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-27329d9e86e297061af9abb979f64a94_1440w.jpg"></div><span class="title">一站看尽NeurIPS 2025前沿成果，11月22日北京见！</span></a><a class="item" href="/rss/87c0a85e.html" title="世界模型可单GPU秒级生成了？腾讯开源FlashWorld，效果惊艳、免费体验"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pica.zhimg.com/v2-b8803d9ec302a4a8077f238d25cc4b4c_1440w.jpg"></div><span class="title">世界模型可单GPU秒级生成了？腾讯开源FlashWorld，效果惊艳、免费体验</span></a><a class="item" href="/rss/881f08a3.html" title="AI版盗梦空间？Claude竟能察觉到自己被注入概念了"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pica.zhimg.com/v2-a24ab3f367283fd1b5f06854c1182198_1440w.jpg"></div><span class="title">AI版盗梦空间？Claude竟能察觉到自己被注入概念了</span></a><a class="item" href="/rss/ec2e4bc.html" title="人大、清华DeepAnalyze，让LLM化身数据科学家"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pica.zhimg.com/v2-b8803d9ec302a4a8077f238d25cc4b4c_1440w.jpg"></div><span class="title">人大、清华DeepAnalyze，让LLM化身数据科学家</span></a><a class="item" href="/rss/9620cacc.html" title="刚刚，智源悟界·Emu3.5登场，原生具备世界建模能力"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-b7204d094a031d3683f4e994f200ee92.jpg?source=382ee89a"></div><span class="title">刚刚，智源悟界·Emu3.5登场，原生具备世界建模能力</span></a><a class="item" href="/rss/2a0de12e.html" title="单张4090跑到30fps，范浩强团队让VLA实时跑起来了"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-b8803d9ec302a4a8077f238d25cc4b4c_720w.jpg?source=d16d100b"></div><span class="title">单张4090跑到30fps，范浩强团队让VLA实时跑起来了</span></a><a class="item" href="/rss/e1808919.html" title="港科提出新算法革新大模型推理范式：随机策略估值竟成LLM数学推理「神操作」"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-b8803d9ec302a4a8077f238d25cc4b4c_720w.jpg?source=d16d100b"></div><span class="title">港科提出新算法革新大模型推理范式：随机策略估值竟成LLM数学推理「神操作」</span></a><a class="item" href="/rss/e7f64a91.html" title="重新定义跨模态生成的流匹配范式，VAFlow让视频「自己发声」"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-b8803d9ec302a4a8077f238d25cc4b4c_720w.jpg?source=d16d100b"></div><span class="title">重新定义跨模态生成的流匹配范式，VAFlow让视频「自己发声」</span></a><a class="item" href="/rss/ca5454bb.html" title="AI百科全书SciencePedia：当马斯克Grokipedia遭遇滑铁卢，有个中国团队默默把活儿干了"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-90b8d9d2db6036666ec62b38dba81e0f_1440w.jpg"></div><span class="title">AI百科全书SciencePedia：当马斯克Grokipedia遭遇滑铁卢，有个中国团队默默把活儿干了</span></a><a class="item" href="/rss/4b347ad2.html" title="OpenAI首个GPT-5找Bug智能体：全自动读代码找漏洞写修复"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic2.zhimg.com/v2-c3d5a9680236638717542c097d62d427_1440w.jpg"></div><span class="title">OpenAI首个GPT-5找Bug智能体：全自动读代码找漏洞写修复</span></a><a class="item" href="/rss/e8e18acd.html" title="Sora连更三大新功能！一键打造IP形象，限时免注册码抢占安卓市场"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-227408c45b662a96d699e950c7d84d4a_720w.jpg?source=d16d100b"></div><span class="title">Sora连更三大新功能！一键打造IP形象，限时免注册码抢占安卓市场</span></a></div></section></div>





      
<footer class="page-footer reveal fs12"><hr><div class="sitemap"><div class="sitemap-group"><span class="fs14">RSS</span><a href="/">近期</a><a href="/categories/">分类</a><a href="/tags/">标签</a><a href="/archives/">归档</a></div><div class="sitemap-group"><span class="fs14">Support</span><a target="_blank" rel="noopener" href="https://blog.mhuig.top/">博客</a><a target="_blank" rel="noopener" href="https://api.mhuig.top/">API</a><a target="_blank" rel="noopener" href="https://ssl.mhuig.top/">SSL Status</a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://mhuig.instatus.com/">Status Monitors</a></div><div class="sitemap-group"><span class="fs14">社交</span><a target="_blank" rel="noopener" href="https://blog.mhuig.top/pages/friends/">友链</a><a target="_blank" rel="noopener" href="https://blog.mhuig.top/pages/about/">留言板</a></div><div class="sitemap-group"><span class="fs14">更多</span><a target="_blank" rel="noopener" href="https://mhuig.top/">关于我</a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/MHuiG">GitHub</a><a href="/contact/">Contact</a><a href="/privacy-policy/">隐私政策</a></div></div><div class="text"><p>本站由 <a target="_blank" rel="noopener" href="https://mhuig.top/">MHuiG</a> 使用 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/xaoxuu/hexo-theme-stellar">Stellar</a> 主题创建，您可以在 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/MHG-LAB/RSSBOX">GitHub</a> 找到本站源码<br>本博客所有文章除特别声明外，均采用 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/me-shaon/GLWTPL/blob/master/translations/LICENSE_zh-CN">GLWT（Good Luck With That，祝你好运）公共许可证</a> 许可协议，转载请注明出处。</p>
</div></footer>

      <div class="float-panel mobile-only blur" style="display:none">
  <button type="button" class="sidebar-toggle mobile" onclick="sidebar.toggle()">
    <svg class="icon" style="width: 1em; height: 1em;vertical-align: middle;fill: currentColor;overflow: hidden;" viewbox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15301"><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 2.3 26.8 24.6 47.5 51.6 47.6h416.5v4z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15302"/><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 1.9 27.7 23.9 49.7 51.6 51.6h416.5z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15303"/></svg>
  </button>
</div>

    </div>
  </div>
  <div class="scripts">
    <script type="text/javascript">
  stellar = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    loadCSS: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    // 从 butterfly 和 volantis 获得灵感
    loadScript: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    // https://github.com/jerryc127/hexo-theme-butterfly
    jQuery: (fn) => {
      if (typeof jQuery === 'undefined') {
        stellar.loadScript(stellar.plugins.jQuery).then(fn)
      } else {
        fn()
      }
    }
  };
  stellar.github = 'https://github.com/xaoxuu/hexo-theme-stellar/tree/1.9.0';
  stellar.config = {
    date_suffix: {
      just: '刚刚',
      min: '分钟前',
      hour: '小时前',
      day: '天前',
      month: '个月前',
    },
  };

  // required plugins (only load if needs)
  stellar.plugins = {
    jQuery: 'https://static.mhuig.top/npm/jquery@3.5.1/dist/jquery.min.js',
    sitesjs: '/js/plugins/sites.js',
    friendsjs: '/js/plugins/friends.js',
  };

  // optional plugins
  if ('true' == 'true') {
    stellar.plugins.lazyload = Object.assign({"enable":true,"js":"https://static.mhuig.top/npm/vanilla-lazyload@17.3.1/dist/lazyload.min.js","transition":"blur"});
  }
  if ('true' == 'true') {
    stellar.plugins.swiper = Object.assign({"enable":true,"css":"https://unpkg.com/swiper@6/swiper-bundle.min.css","js":"https://unpkg.com/swiper@6/swiper-bundle.min.js"});
  }
  if ('' == 'true') {
    stellar.plugins.scrollreveal = Object.assign({"enable":null,"js":"https://static.mhuig.top/npm/scrollreveal@4.0.9/dist/scrollreveal.min.js","distance":"8px","duration":500,"interval":100,"scale":1});
  }
  if ('true' == 'true') {
    stellar.plugins.preload = Object.assign({"enable":true,"service":"flying_pages","instant_page":"https://static.mhuig.top/gh/volantis-x/cdn-volantis@4.1.2/js/instant_page.js","flying_pages":"https://static.mhuig.top/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"});
  }
  if ('true' == 'true') {
    stellar.plugins.fancybox = Object.assign({"enable":true,"js":"https://static.mhuig.top/npm/@fancyapps/ui@4.0/dist/fancybox.umd.js","css":"https://static.mhuig.top/npm/@fancyapps/ui@4.0/dist/fancybox.css","selector":".swiper-slide img,article.content img"});
  }
  if ('false' == 'true') {
    stellar.plugins.heti = Object.assign({"enable":false,"css":"https://unpkg.com/heti/umd/heti.min.css","js":"https://unpkg.com/heti/umd/heti-addon.min.js"});
  }
</script>

<!-- required -->

  
<script src="/js/main.js" async></script>



<!-- optional -->



<!-- inject -->


  </div>
</body>
</html>
