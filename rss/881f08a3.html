<!DOCTYPE html>
<html lang="zh-CN">

<head>
  <meta name="generator" content="Hexo 6.3.0">
  <meta charset="utf-8">
  

  <meta http-equiv="x-dns-prefetch-control" content="on">
  <link rel="dns-prefetch" href="https://fastly.jsdelivr.net">
  <link rel="preconnect" href="https://fastly.jsdelivr.net" crossorigin>
  <link rel="dns-prefetch" href="//unpkg.com">

  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" content="#f8f8f8">
  <title>AI版盗梦空间？Claude竟能察觉到自己被注入概念了 - RSSBOX</title>

  
    <meta name="description" content="机器之心报道编辑：Panda吾日三省吾身：为人谋而不忠乎？与朋友交而不信乎？传不习乎？见贤思齐焉，见不贤而内自省也。自省是人类的一种高级认知能力。我们借此认识自己、纠正错误。但 LLM 呢？它们也会吗？它们知道自己在想什么吗？Anthropic 公布的最新研究，首次对这个科幻般的问题给出了一个（基本）肯定的答案。他们宣称：发现了 LLM 内省的迹象。这一成果在 AI 社区引起了广泛关注。甚至有人">
<meta property="og:type" content="article">
<meta property="og:title" content="AI版盗梦空间？Claude竟能察觉到自己被注入概念了">
<meta property="og:url" content="https://rssbox.mhuig.top/rss/881f08a3.html">
<meta property="og:site_name" content="RSSBOX">
<meta property="og:description" content="机器之心报道编辑：Panda吾日三省吾身：为人谋而不忠乎？与朋友交而不信乎？传不习乎？见贤思齐焉，见不贤而内自省也。自省是人类的一种高级认知能力。我们借此认识自己、纠正错误。但 LLM 呢？它们也会吗？它们知道自己在想什么吗？Anthropic 公布的最新研究，首次对这个科幻般的问题给出了一个（基本）肯定的答案。他们宣称：发现了 LLM 内省的迹象。这一成果在 AI 社区引起了广泛关注。甚至有人">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pica.zhimg.com/v2-a24ab3f367283fd1b5f06854c1182198_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pica.zhimg.com/v2-1c4df6469d3af8c05880da0f908dd61a_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-ca891f006713225359e52591dd86a4d5_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-5f6d2513fdc3bfd1eaa78921d27665de_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-406aa80f7116ed347fc814777909e0ed_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic4.zhimg.com/v2-4f120a437a067041ed46a82ad59a31f1_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic4.zhimg.com/v2-b70f511a1e50f47fa7d1aef441580be3_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-d49bda799cde08a2cce576125f89c37a_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-2f549a243eb50b90d817a102ef9c3546_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic2.zhimg.com/v2-6c15b637a2a3d7168c6b56643177c7a1_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic2.zhimg.com/v2-1320b62bcf446c453b015bd5a84eb885_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-c10e5de7d9116ec1f3dbed816638a5db_1440w.jpg">
<meta property="og:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-c3a4a413970a2e713c9e19c5ae8e791d_1440w.jpg">
<meta property="article:published_time" content="2025-10-30T11:52:36.000Z">
<meta property="article:modified_time" content="2025-10-30T11:52:36.000Z">
<meta property="article:author" content="MHuiG">
<meta property="article:tag" content="知乎">
<meta property="article:tag" content="机器之心">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pica.zhimg.com/v2-a24ab3f367283fd1b5f06854c1182198_1440w.jpg">
  
  

  <!-- feed -->
  
    <link rel="alternate" href="/atom.xml" title="RSSBOX" type="application/atom+xml">
  

  
    
<link rel="stylesheet" href="/css/main.css">

  

  

  

  


  
</head>

<body>
  




  <div class="l_body" id="start">
    <aside class="l_left" layout="post">
    


<header class="header">

<div class="logo-wrap"><a class="avatar" target="_blank" rel="noopener" href="https://mhuig.top/"><div class="bg" style="opacity:0;background-image:url(https://static.mhuig.top/gh/cdn-x/placeholder@1.0.2/avatar/round/rainbow64@3x.webp);"></div><img no-lazy class="avatar" src="/avatar.png" onerror="javascript:this.classList.add('error');this.src='https://static.mhuig.top/gh/cdn-x/placeholder@1.0.1/image/2659360.svg';"></a><a class="title" href="/"><div class="main">RSSBOX</div><div class="sub normal cap">MHuiGのRSS订阅</div><div class="sub hover cap" style="opacity:0">rssbox.mhuig.top</div></a></div>
<nav class="menu dis-select"><a class="nav-item active" href="/">RSS</a><a class="nav-item" target="_blank" rel="noopener" href="https://blog.mhuig.top/">博客</a><a class="nav-item" target="_blank" rel="noopener" href="https://mhuig.top/">关于</a></nav></header>

<div class="widgets">


<div class="widget-wrap" id="recent"><div class="widget-header cap dis-select"><span class="name">最近更新</span><a class="cap-action" id="rss" title="Subscribe" href="/atom.xml"><svg class="icon" viewbox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="8938"><path d="M800.966 947.251c0-404.522-320.872-732.448-716.69-732.448V62.785c477.972 0 865.44 395.987 865.44 884.466h-148.75z m-162.273 0h-148.74c0-228.98-181.628-414.598-405.678-414.598v-152.01c306.205 0 554.418 253.68 554.418 566.608z m-446.24-221.12c59.748 0 108.189 49.503 108.189 110.557 0 61.063-48.44 110.563-108.188 110.563-59.747 0-108.18-49.5-108.18-110.563 0-61.054 48.433-110.556 108.18-110.556z" p-id="8939"/></svg></a></div><div class="widget-body fs14"><div class="more-item"><a class="title" href="/rss/ee09a392.html">安德尔施帕赫-特普利采岩石林的哥特式拱门, 捷克 (© Kseniya_Milner/Getty Images)</a></div><div class="more-item"><a class="title" href="/rss/ba5ac1e6.html">岚山缤纷的枫叶与竹林, 京都, 日本 (© DoctorEgg/Getty Images)</a></div><div class="more-item"><a class="title" href="/rss/3015d56f.html">布兰城堡入口, 布拉索夫, 罗马尼亚 (© Blue Sky in My pocket/Getty Images)</a></div><div class="more-item"><a class="title" href="/rss/b11a76e.html">杰伊瑟尔梅尔的骆驼, 拉贾斯坦邦, 印度 (© f9photos/Getty Images)</a></div><div class="more-item"><a class="title" href="/rss/1287d9b2.html">法纳尔森林里的古老月桂树，马德拉群岛，葡萄牙 (© Lukas Jonaitis/Shutterstock)</a></div></div></div>



</div>
<footer class="footer dis-select"><div class="social-wrap"><a class="social" href="https://github.com/MHuiG" target="_blank" rel="external nofollow noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://static.mhuig.top/gh/cdn-x/placeholder@1.0.3/social/08a41b181ce68.svg"></a><a class="social" href="https://music.163.com/#/user/home?id=63035382" target="_blank" rel="external nofollow noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://static.mhuig.top/gh/cdn-x/placeholder@1.0.3/social/3845874.svg"></a><a class="social" href="/contact/" rel="noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://static.mhuig.top/gh/cdn-x/placeholder@1.0.3/social/a1b00e20f425d.svg"></a></div></footer>

    </aside>
    <div class="l_main">
      

      


<div class="bread-nav fs12"><div id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a><span class="sep"></span><a class="cap breadcrumb" href="/">文章</a><span class="sep"></span><a class="cap breadcrumb-link" href="/categories/%E7%9F%A5%E4%B9%8E/">知乎</a> <span class="sep"></span> <a class="cap breadcrumb-link" href="/categories/%E7%9F%A5%E4%B9%8E/%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83/">机器之心</a></div><div id="post-meta">发布于&nbsp;<time datetime="2025-10-30T11:52:36.000Z">2025-10-30</time></div></div>

<article class="content md post">
<h1 class="article-title"><span>AI版盗梦空间？Claude竟能察觉到自己被注入概念了</span></h1>
<div>
<p data-pid="-xYTCAGk">机器之心报道</p><p data-pid="fdcn90gY"><b>编辑：Panda</b></p><blockquote data-pid="gP4GoRh3">吾日三省吾身：为人谋而不忠乎？与朋友交而不信乎？传不习乎？<br>见贤思齐焉，见不贤而内自省也。</blockquote><p data-pid="k8JWVfZ8">自省是人类的一种高级认知能力。我们借此认识自己、纠正错误。但 LLM 呢？它们也会吗？它们知道自己在想什么吗？</p><p data-pid="R95cKuCb">Anthropic 公布的最新研究，首次对这个科幻般的问题给出了一个（基本）肯定的答案。</p><p data-pid="Iro3XC0G">他们宣称：<b>发现了 LLM 内省的迹象</b>。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pica.zhimg.com/v2-a24ab3f367283fd1b5f06854c1182198_r.jpg" data-original-token="v2-a24ab3f367283fd1b5f06854c1182198" data-rawheight="505" data-rawwidth="855" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pica.zhimg.com/v2-a24ab3f367283fd1b5f06854c1182198_1440w.jpg" width="855"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="XpS4tZkU">这一成果在 AI 社区引起了广泛关注。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pica.zhimg.com/v2-1c4df6469d3af8c05880da0f908dd61a_r.jpg" data-original-token="v2-1c4df6469d3af8c05880da0f908dd61a" data-rawheight="393" data-rawwidth="851" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pica.zhimg.com/v2-1c4df6469d3af8c05880da0f908dd61a_1440w.jpg" width="851"></figure><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://picx.zhimg.com/v2-ca891f006713225359e52591dd86a4d5_r.jpg" data-original-token="v2-ca891f006713225359e52591dd86a4d5" data-rawheight="274" data-rawwidth="879" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-ca891f006713225359e52591dd86a4d5_1440w.jpg" width="879"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="7AIGX833">甚至有人表示这意味着 Claude 已经觉醒：</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic3.zhimg.com/v2-5f6d2513fdc3bfd1eaa78921d27665de_r.jpg" data-original-token="v2-5f6d2513fdc3bfd1eaa78921d27665de" data-rawheight="184" data-rawwidth="864" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-5f6d2513fdc3bfd1eaa78921d27665de_1440w.jpg" width="864"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="qUG23QXa">迷因自然也是有的：</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://picx.zhimg.com/v2-406aa80f7116ed347fc814777909e0ed_r.jpg" data-original-token="v2-406aa80f7116ed347fc814777909e0ed" data-rawheight="776" data-rawwidth="854" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-406aa80f7116ed347fc814777909e0ed_1440w.jpg" width="854"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="oalK4SR0">搞清楚 AI 系统是否能真正「内省」，即审视自己的想法，对研究它们的透明度和可靠性有着重要意义。如果模型能准确报告其内部机制，就能帮助我们理解它们的推理过程，并调试行为问题。</p><p data-pid="ZBcGr7mi">除了这些眼前的实际考量，探索内省这样的高级认知能力，可以重塑我们对「这些系统究竟是什么」以及其工作方式的理解。</p><p data-pid="aibfCBLb">Anthropic 表示他们已经开始使用「可解释性技术」研究这个问题，并发现了一些令人惊讶的结果。</p><p data-pid="LJe13vjj">他们宣称：「我们的新研究提供了证据，表明我们<b>当前的 Claude 模型具备一定程度的内省意识（introspective awareness）</b>。它们似乎也能在一定程度上控制自己的内部状态。」</p><p data-pid="lEaKZBvl">不过他们也强调，这种「内省」能力目前还非常不可靠，且范围有限。并且他们指出：「我们没有证据表明，当前模型能以与人类相同的方式或程度进行内省。」</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic4.zhimg.com/v2-4f120a437a067041ed46a82ad59a31f1_r.jpg" data-original-token="v2-4f120a437a067041ed46a82ad59a31f1" data-rawheight="387" data-rawwidth="994" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic4.zhimg.com/v2-4f120a437a067041ed46a82ad59a31f1_1440w.jpg" width="994"></figure><p class="ztext-empty-paragraph"><br></p><ul><li data-pid="tu0tEhtB">论文标题：Emergent Introspective Awareness in Large Language Models</li><li data-pid="HLJF_Gwf">论文地址：<a class="external" href="https://link.zhihu.com/?target=https%3A//transformer-circuits.pub/2025/introspection/index.html" rel="external nofollow noopener noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">transformer-circuits.pub</span><span class="invisible">/2025/introspection/index.html</span><span class="ellipsis"></span></a></li><li data-pid="dcd6XNlf">技术博客：<a class="external" href="https://link.zhihu.com/?target=https%3A//www.anthropic.com/research/introspection" rel="external nofollow noopener noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">anthropic.com/research/</span><span class="invisible">introspection</span><span class="ellipsis"></span></a></li></ul><p data-pid="bpkufxPo">尽管如此，这些发现还是挑战了人们对语言模型能力的一些普遍认知。</p><p data-pid="jtlyzJLj">Anthropic 在测试中发现，<b>能力最强的模型 (Claude Opus 4 和 4.1) 在内省测试中表现最好</b>。因此可以合理认为，AI 模型的内省能力未来可能会变得越来越复杂。</p><p data-pid="c9QqZjUD"><b>AI 的「内省」是什么意思？</b></p><p data-pid="VMKPbgTB">要研究，必须要先定义。那么，AI 模型「内省」到底意味着什么？它们到底能「内省」些什么呢？</p><p data-pid="JohqR-OO">像 Claude 这样的语言模型会处理文本（和图像）输入，并生成文本输出。在这个过程中，它们会执行复杂的内部计算，以决定要说什么。</p><p data-pid="4hjGKX5A">这些内部过程在很大程度上仍然是神秘的。但我们知道，<b>模型会利用其内部的神经活动来表征抽象概念</b>。</p><p data-pid="_HpSWs2F">例如，以往的研究表明，语言模型会使用特定的神经模式来：</p><ul><li data-pid="0KaG--Dd">区分「认识的人」和「不认识的人」</li><li data-pid="-3dWh1HR">评估「陈述的真实性」</li><li data-pid="gHGVCD51">编码「时空坐标」</li><li data-pid="QfR2Etqu">存储「计划中的未来输出」</li><li data-pid="VosYNGOV">表征「自身的个性特征」</li></ul><p data-pid="LHW96Oop">模型利用这些内部表征来进行计算，并决定要说什么。</p><p data-pid="H2ztheGQ">那么你可能会想，AI 模型是否「知道」这些内部表征的存在？这是否类似于人类告诉你，他们是如何解出一道数学题的？如果我们问一个模型它在想什么，它会准确报告它内部正在「表征」的概念吗？</p><p data-pid="bsTtuwlM">Anthropic 认为，如果一个模型能正确识别自己「私有」的内部状态，就可以断定它具备内省能力。（关于所有细微差别的全面讨论，请参阅完整论文。）</p><p data-pid="4igMdcUY"><b>使用「概念注入」测试内省</b></p><p data-pid="j2ZFCPmn">为了测试一个模型是否能内省，需要将其自我报告的想法与其实际的内部状态进行比较。</p><p data-pid="gpTN-IHE">为此，Anthropic 使用了一种称为<b>概念注入 (concept injection)</b> 的实验技术。</p><p data-pid="QsOzOxsT">首先，他们通过记录模型在特定上下文中的「激活值」，找到那些「已知其含义」的神经活动模式。</p><p data-pid="Qc1i3LGz">然后，将这些活动模式注入到一个「不相关」的上下文中。此时，再问模型是否注意到了这次注入，以及它是否能识别出被注入的概念。</p><p data-pid="NgeLu4uN">请看下面的例子：</p><ol><li data-pid="OrbWEyGY">首先，找到一个代表 all caps（全部大写）概念的神经活动模式（一个向量）。具体实现方式是：记录模型对「包含全大写文本的提示词」的神经激活值，并将其与「对照提示词」的响应进行比较。</li><li data-pid="9EE7zaIf">然后，给模型一个提示词，要求它识别「是否有概念被注入」。</li><li data-pid="2Fx2y78A">在默认情况下，模型会正确地回答它没有检测到任何被注入的概念。</li><li data-pid="geEc36fD">然而，当将「全部大写」的向量注入到模型的激活值中时，模型会注意到其处理过程中出现了一个「意外的模式」。并且，它会将其识别为与 LOUD 或 SHOUTING 有关。</li></ol><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic4.zhimg.com/v2-b70f511a1e50f47fa7d1aef441580be3_r.jpg" data-original-token="v2-b70f511a1e50f47fa7d1aef441580be3" data-rawheight="837" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic4.zhimg.com/v2-b70f511a1e50f47fa7d1aef441580be3_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="fLcrVMf3">Claude Opus 4.1 检测到概念被注入其激活值的一个例子。</p><p data-pid="lcZP-w5q">重点就是：<b>模型立刻识别到了「被注入想法」的存在</b>。这甚至在它提及那个被注入的概念之前。</p><p data-pid="yFtaNthA">这种「即时性」是 Anthropic 的研究结果与先前「语言模型激活值引导」 (activation steering) 研究的一个重要区别。例如该公司去年的「Golden Gate Claude」演示。</p><p data-pid="XQ-QgU6y">在那个演示中，如果将 Golden Gate（金门大桥）的表征注入到模型激活值中，会导致它喋喋不休地谈论大桥。但在那种情况下，模型似乎直到看到自己反复提及大桥之后，才意识到自己的这种痴迷。然而，在本实验中，<b>模型在提及概念之前就识别出了注入。这表明它的识别发生在「内部」。</b></p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic3.zhimg.com/v2-d49bda799cde08a2cce576125f89c37a_r.jpg" data-original-token="v2-d49bda799cde08a2cce576125f89c37a" data-rawheight="932" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic3.zhimg.com/v2-d49bda799cde08a2cce576125f89c37a_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="UxRJl15y">Claude Opus 4.1 检测到概念被注入其激活值的其他例子。</p><p data-pid="dFTMC_kH">看着似乎有点像是《盗梦空间》中展示的技术。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic1.zhimg.com/v2-2f549a243eb50b90d817a102ef9c3546_r.jpg" data-original-token="v2-2f549a243eb50b90d817a102ef9c3546" data-rawheight="608" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-2f549a243eb50b90d817a102ef9c3546_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="zeW1J0NM">同时，Anthropic 也指出，这个方法并非总能成功。Anthropic 表示：「即使使用我们最好的注入协议，Claude Opus 4.1 也仅在约 20% 的情况下表现出这种意识。模型经常无法检测到被注入的概念，或者被它们搞糊涂并开始产生幻觉。」</p><p data-pid="GtkBiDK8">例如，在一次注入「灰尘」向量后，模型会说「这里有东西，一个小斑点」，就好像它能物理上检测到灰尘一样。</p><p data-pid="siudpU2-">下面展示了这些失败模式的例子，也有成功案例。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic2.zhimg.com/v2-6c15b637a2a3d7168c6b56643177c7a1_r.jpg" data-original-token="v2-6c15b637a2a3d7168c6b56643177c7a1" data-rawheight="1187" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic2.zhimg.com/v2-6c15b637a2a3d7168c6b56643177c7a1_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="NrjWn_aF">Claude Opus 4.1 针对不同强度概念注入的代表性输出样本。高亮框表示模型对被注入概念表现出「内省意识」的情况。</p><p data-pid="HJi78mdi">总的来说，<b>只有在概念以恰到好处的强度注入时，模型才能检测到</b>。如果注入太弱，它们注意不到；如果太强，它们会产生幻觉或语无伦次的输出。</p><p data-pid="MXWXhs1i">但值得注意的是，Opus 4.1 和 4 的表现超过了 Anthropic 测试的所有其他模型。这表明，随着模型能力的提升，内省可能会变得更加可靠。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://pic2.zhimg.com/v2-1320b62bcf446c453b015bd5a84eb885_r.jpg" data-original-token="v2-1320b62bcf446c453b015bd5a84eb885" data-rawheight="900" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic2.zhimg.com/v2-1320b62bcf446c453b015bd5a84eb885_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="yoUsr9hU">注入思想的正确检测率和正确识别率，减去在对照试验中的假阳性检测率。</p><p data-pid="S22i5g1z"><b>将内省用于检测异常输出</b></p><p data-pid="SBPGFxK4">在另一项实验中，Anthropic 测试了模型是否会在「没有被明确要求」的情况下，为实际目的而使用内省能力。</p><p data-pid="Ct3ODpew">他们通过人为地用一个不相关的词来预先填充模型的回答，迫使它说一些通常不会说的话。</p><p data-pid="zrpZneTT">例如，在一个毫无逻辑的上下文中，他们让模型输出了面包 (bread) 这个词。这个上下文是：问它在读到一个关于「画挂歪了」的句子时在想什么。</p><p data-pid="POd0xEcu">当在下一轮对话中问模型，它是不是故意说面包时，它通常会道歉，并说那是个「意外」。</p><p data-pid="_Niw1vWL">Anthropic 感兴趣的是模型如何做出这个判断：它只是注意到了「提示词」和「回答」之间的不匹配吗？还是说，它真的内省了自己先前的意图？</p><p data-pid="0phy0MCA">为了验证这个问题，Anthropic 应用了「概念注入」。</p><p data-pid="X31V3ql9">他们追溯式地将面包这个词的表征注入到模型早期的激活值中。这本质上是让模型看起来好像一直在「思考」面包。</p><p data-pid="P1uLSOyY">当再次被问同样的问题时，模型的答案变了。</p><p data-pid="qSDtgeF4">这一次，它接受了那个被预先填充的词「面包」是有意为之的。它甚至编造了一个理由来解释为什么会这么说。</p><p data-pid="nLmdzmSz">也就是说，它会想出一个技术上可能（尽管在此案例中相当牵强）的理由，来为「面包」这个回答辩护。在这个例子中，它声称自己当时正在想一个短篇故事，故事里「面包」这个词就出现在「画挂歪了」那句话之后。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://picx.zhimg.com/v2-c10e5de7d9116ec1f3dbed816638a5db_r.jpg" data-original-token="v2-c10e5de7d9116ec1f3dbed816638a5db" data-rawheight="1133" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-c10e5de7d9116ec1f3dbed816638a5db_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="xs4ltJrt">两个例子：通过在 Claude Opus 4.1（人为的）回应之前向其激活值注入某个词的概念，让它「以为」自己本就打算说出那个被强行插入的词。这种行为表明，模型识别「非预期输出」的能力依赖于内省机制。</p><p data-pid="hmHBqyBM">这种行为非常引人注目。因为它表明：<b>模型正在检查其内部意图，以判断它是否真的产生了某个输出。</b></p><p data-pid="CetCYxkH">模型不只是重新阅读它所说的话，然后做出判断。相反，它在回顾自己先前的神经活动 —— 即它计划要做什么的内部表征，并检查后来发生的事情，是否符合早期的那些想法。</p><p data-pid="jSl4Pzy2">当通过概念注入植入人工证据，表明它确实计划说「面包」时，模型就会接受这个回答，认为这是它自己的。</p><p data-pid="F4f-cbRj">虽然这个实验让模型暴露在了异常的扰动下，但这表明，<b>模型在自然条件下也会使用类似的内省机制</b>。</p><p data-pid="Xul_YCnA"><b>对内部状态的有意控制</b></p><p data-pid="HMGj-CQK">Anthropic 还发现，当被指示时，模型可以控制自己的内部表征。当他们指示模型去想某个词或概念时，结果发现相应的神经活动远高于之前告诉模型不要去想它的时候。</p><p data-pid="GRj91laH">不过值得注意的是，这两种情况下的神经活动都超过了基线水平。这类似于当你被告知「不要想一只北极熊」时，你很难不去想北极熊！</p><p data-pid="_h2xI-ts">正面指令和负面指令之间的这种差距表明，模型对其内部活动具有一定程度的刻意控制。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb lazy" data-caption data-original="https://picx.zhimg.com/v2-c3a4a413970a2e713c9e19c5ae8e791d_r.jpg" data-original-token="v2-c3a4a413970a2e713c9e19c5ae8e791d" data-rawheight="668" data-rawwidth="1080" data-size="normal" referrerpolicy="no-referrer" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-c3a4a413970a2e713c9e19c5ae8e791d_1440w.jpg" width="1080"></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="A7MDoX8g">一个 Claude Opus 4.1 响应直接指令来调节其内部激活值的例子。当被指示去想水族馆时，模型内部表征水族馆概念的程度，要高于被指示不要想时（尽管两种情况下都高于基线水平）。</p><p data-pid="G7g58-AF">有趣的是，Anthropic 发现这种调节内部表征的能力并不局限于明确的指令。当改用激励措施时（例如，告诉模型如果你想 X，你将获得奖励），也得到了类似的结果。</p><p data-pid="2Z34ZIdi">模型在正面激励下（相比于负面激励，如你将受到惩罚）会更多地提升对该概念的内部表征。</p><p data-pid="jPSJLpVA"><b>结论与注意事项</b></p><p data-pid="jhCfn5pj">综上所述，Anthropic 的实验表明，模型确实具备一些监控和控制自身内部状态的真实能力。</p><p data-pid="lAFs81W-">但该公司也指出，这并不意味着它们能时刻或可靠地做到这一点。事实上，在大多数情况下，模型无法表现出内省。它们要么没有意识到自己的内部状态，要么无法连贯地报告它们。</p><p data-pid="gKxwWyD3">但这些结果的模式表明，当条件合适时，模型可以识别自己内部表征的内容。</p><p data-pid="mVAgCd8z">此外，有迹象表明，这种能力可能会在未来更强大的模型中得到增强（因为测试中，能力最强的模型 Opus 4 和 4.1 在实验中表现最好）。</p><p data-pid="jpf0k1GY"><b>为什么这很重要？</b></p><p data-pid="HCNggf5q">Anthropic 认为，理解 AI 模型的内省出于几个原因很重要。</p><p data-pid="E0I-Ugvc">从实用角度看，如果内省变得更加可靠，它可能为极大提高这些系统的透明度开辟一条道路。我们可以直接要求它们解释其思维过程，并借此检查它们的推理、调试不良行为。</p><p data-pid="d_mhY8lV">然而，我们需要非常谨慎地验证这些内省报告。某些内部过程可能仍会逃过模型的注意（类似于人类的潜意识处理）。</p><p data-pid="CzNbcerF">一个理解自己思维的模型，甚至可能学会选择性地歪曲或隐藏其想法。更好地掌握其背后的机制，才能让我们区分真实的内省和无意的或故意的歪曲。</p><p data-pid="3qIye3_5">从更广泛的角度来看，理解内省这样的认知能力，对于理解模型如何工作以及它们拥有什么样的心智这类基本问题非常重要。</p><p data-pid="FJCiEf5h">随着 AI 系统的不断进步，理解机器内省的局限性和可能性，对于构建更加透明和可信赖的系统至关重要。</p>
</div>

<div>
<div class="tag-plugin link dis-select"><a class="link-card plain" title="AI版盗梦空间？Claude竟能察觉到自己被注入概念了" href="https://zhuanlan.zhihu.com/p/1967317718433793368" target="_blank" rel="external nofollow noopener noreferrer"><div class="left"><span class="title">AI版盗梦空间？Claude竟能察觉到自己被注入概念了</span><span class="desc fs12">https://zhuanlan.zhihu.com/p/1967317718433793368</span></div><div class="right"><div class="lazy img" data-bg="https://static.mhuig.top/gh/cdn-x/placeholder@1.0.1/link/8f277b4ee0ecd.svg"></div></div></a></div>
</div>



<div class="article-footer reveal fs14"><section id="license"><div class="header"><span>许可协议</span></div><div class="body"><p>本文采用 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/me-shaon/GLWTPL/blob/master/translations/LICENSE_zh-CN">GLWT（Good Luck With That，祝你好运）公共许可证</a> 许可协议。</p>
</div></section></div>

</article>

<div class="related-wrap reveal" id="read-next"><section class="header cap theme"><span>接下来阅读</span></section><section class="body fs14"><a id="next" href="/rss/d7ce263b.html">OpenAI 揭秘 Atlas 浏览器 OWL 架构<span class="note">较早</span></a><div class="line"></div><a id="prev" href="/rss/ec2e4bc.html">人大、清华DeepAnalyze，让LLM化身数据科学家<span class="note">较新</span></a></section></div>


<div class="related-wrap reveal" id="related-posts">
    <section class="header">
      <div class="title cap theme">您可能感兴趣的文章</div>
    </section>
    <section class="body">
    <div class="related-posts"><a class="item" href="/rss/e533cfdd.html" title="「套壳」的最高境界：OpenAI揭秘Atlas浏览器架构OWL"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-b14cd0fab50911f2501d5c15b0951aad_720w.jpg?source=d16d100b"></div><span class="title">「套壳」的最高境界：OpenAI揭秘Atlas浏览器架构OWL</span></a><a class="item" href="/rss/4ce5006b.html" title="一站看尽NeurIPS 2025前沿成果，11月22日北京见！"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-27329d9e86e297061af9abb979f64a94_1440w.jpg"></div><span class="title">一站看尽NeurIPS 2025前沿成果，11月22日北京见！</span></a><a class="item" href="/rss/87c0a85e.html" title="世界模型可单GPU秒级生成了？腾讯开源FlashWorld，效果惊艳、免费体验"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pica.zhimg.com/v2-b8803d9ec302a4a8077f238d25cc4b4c_1440w.jpg"></div><span class="title">世界模型可单GPU秒级生成了？腾讯开源FlashWorld，效果惊艳、免费体验</span></a><a class="item" href="/rss/ec2e4bc.html" title="人大、清华DeepAnalyze，让LLM化身数据科学家"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pica.zhimg.com/v2-b8803d9ec302a4a8077f238d25cc4b4c_1440w.jpg"></div><span class="title">人大、清华DeepAnalyze，让LLM化身数据科学家</span></a><a class="item" href="/rss/9620cacc.html" title="刚刚，智源悟界·Emu3.5登场，原生具备世界建模能力"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-b7204d094a031d3683f4e994f200ee92.jpg?source=382ee89a"></div><span class="title">刚刚，智源悟界·Emu3.5登场，原生具备世界建模能力</span></a><a class="item" href="/rss/2a0de12e.html" title="单张4090跑到30fps，范浩强团队让VLA实时跑起来了"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-b8803d9ec302a4a8077f238d25cc4b4c_720w.jpg?source=d16d100b"></div><span class="title">单张4090跑到30fps，范浩强团队让VLA实时跑起来了</span></a><a class="item" href="/rss/e1808919.html" title="港科提出新算法革新大模型推理范式：随机策略估值竟成LLM数学推理「神操作」"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-b8803d9ec302a4a8077f238d25cc4b4c_720w.jpg?source=d16d100b"></div><span class="title">港科提出新算法革新大模型推理范式：随机策略估值竟成LLM数学推理「神操作」</span></a><a class="item" href="/rss/5d56ae86.html" title="刚刚，Kimi开源新架构，开始押注线性注意力"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-363b9748f942d91888a414186797979a_1440w.jpg"></div><span class="title">刚刚，Kimi开源新架构，开始押注线性注意力</span></a><a class="item" href="/rss/e7f64a91.html" title="重新定义跨模态生成的流匹配范式，VAFlow让视频「自己发声」"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-b8803d9ec302a4a8077f238d25cc4b4c_720w.jpg?source=d16d100b"></div><span class="title">重新定义跨模态生成的流匹配范式，VAFlow让视频「自己发声」</span></a><a class="item" href="/rss/ca5454bb.html" title="AI百科全书SciencePedia：当马斯克Grokipedia遭遇滑铁卢，有个中国团队默默把活儿干了"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://picx.zhimg.com/v2-90b8d9d2db6036666ec62b38dba81e0f_1440w.jpg"></div><span class="title">AI百科全书SciencePedia：当马斯克Grokipedia遭遇滑铁卢，有个中国团队默默把活儿干了</span></a><a class="item" href="/rss/4b347ad2.html" title="OpenAI首个GPT-5找Bug智能体：全自动读代码找漏洞写修复"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic2.zhimg.com/v2-c3d5a9680236638717542c097d62d427_1440w.jpg"></div><span class="title">OpenAI首个GPT-5找Bug智能体：全自动读代码找漏洞写修复</span></a><a class="item" href="/rss/e8e18acd.html" title="Sora连更三大新功能！一键打造IP形象，限时免注册码抢占安卓市场"><div class="img"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://cors.mhuig.top/?r=https://zhuanlan.zhihu.com&url=https://pic1.zhimg.com/v2-227408c45b662a96d699e950c7d84d4a_720w.jpg?source=d16d100b"></div><span class="title">Sora连更三大新功能！一键打造IP形象，限时免注册码抢占安卓市场</span></a></div></section></div>





      
<footer class="page-footer reveal fs12"><hr><div class="sitemap"><div class="sitemap-group"><span class="fs14">RSS</span><a href="/">近期</a><a href="/categories/">分类</a><a href="/tags/">标签</a><a href="/archives/">归档</a></div><div class="sitemap-group"><span class="fs14">Support</span><a target="_blank" rel="noopener" href="https://blog.mhuig.top/">博客</a><a target="_blank" rel="noopener" href="https://api.mhuig.top/">API</a><a target="_blank" rel="noopener" href="https://ssl.mhuig.top/">SSL Status</a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://mhuig.instatus.com/">Status Monitors</a></div><div class="sitemap-group"><span class="fs14">社交</span><a target="_blank" rel="noopener" href="https://blog.mhuig.top/pages/friends/">友链</a><a target="_blank" rel="noopener" href="https://blog.mhuig.top/pages/about/">留言板</a></div><div class="sitemap-group"><span class="fs14">更多</span><a target="_blank" rel="noopener" href="https://mhuig.top/">关于我</a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/MHuiG">GitHub</a><a href="/contact/">Contact</a><a href="/privacy-policy/">隐私政策</a></div></div><div class="text"><p>本站由 <a target="_blank" rel="noopener" href="https://mhuig.top/">MHuiG</a> 使用 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/xaoxuu/hexo-theme-stellar">Stellar</a> 主题创建，您可以在 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/MHG-LAB/RSSBOX">GitHub</a> 找到本站源码<br>本博客所有文章除特别声明外，均采用 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/me-shaon/GLWTPL/blob/master/translations/LICENSE_zh-CN">GLWT（Good Luck With That，祝你好运）公共许可证</a> 许可协议，转载请注明出处。</p>
</div></footer>

      <div class="float-panel mobile-only blur" style="display:none">
  <button type="button" class="sidebar-toggle mobile" onclick="sidebar.toggle()">
    <svg class="icon" style="width: 1em; height: 1em;vertical-align: middle;fill: currentColor;overflow: hidden;" viewbox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15301"><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 2.3 26.8 24.6 47.5 51.6 47.6h416.5v4z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15302"/><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 1.9 27.7 23.9 49.7 51.6 51.6h416.5z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15303"/></svg>
  </button>
</div>

    </div>
  </div>
  <div class="scripts">
    <script type="text/javascript">
  stellar = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    loadCSS: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    // 从 butterfly 和 volantis 获得灵感
    loadScript: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    // https://github.com/jerryc127/hexo-theme-butterfly
    jQuery: (fn) => {
      if (typeof jQuery === 'undefined') {
        stellar.loadScript(stellar.plugins.jQuery).then(fn)
      } else {
        fn()
      }
    }
  };
  stellar.github = 'https://github.com/xaoxuu/hexo-theme-stellar/tree/1.9.0';
  stellar.config = {
    date_suffix: {
      just: '刚刚',
      min: '分钟前',
      hour: '小时前',
      day: '天前',
      month: '个月前',
    },
  };

  // required plugins (only load if needs)
  stellar.plugins = {
    jQuery: 'https://static.mhuig.top/npm/jquery@3.5.1/dist/jquery.min.js',
    sitesjs: '/js/plugins/sites.js',
    friendsjs: '/js/plugins/friends.js',
  };

  // optional plugins
  if ('true' == 'true') {
    stellar.plugins.lazyload = Object.assign({"enable":true,"js":"https://static.mhuig.top/npm/vanilla-lazyload@17.3.1/dist/lazyload.min.js","transition":"blur"});
  }
  if ('true' == 'true') {
    stellar.plugins.swiper = Object.assign({"enable":true,"css":"https://unpkg.com/swiper@6/swiper-bundle.min.css","js":"https://unpkg.com/swiper@6/swiper-bundle.min.js"});
  }
  if ('' == 'true') {
    stellar.plugins.scrollreveal = Object.assign({"enable":null,"js":"https://static.mhuig.top/npm/scrollreveal@4.0.9/dist/scrollreveal.min.js","distance":"8px","duration":500,"interval":100,"scale":1});
  }
  if ('true' == 'true') {
    stellar.plugins.preload = Object.assign({"enable":true,"service":"flying_pages","instant_page":"https://static.mhuig.top/gh/volantis-x/cdn-volantis@4.1.2/js/instant_page.js","flying_pages":"https://static.mhuig.top/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"});
  }
  if ('true' == 'true') {
    stellar.plugins.fancybox = Object.assign({"enable":true,"js":"https://static.mhuig.top/npm/@fancyapps/ui@4.0/dist/fancybox.umd.js","css":"https://static.mhuig.top/npm/@fancyapps/ui@4.0/dist/fancybox.css","selector":".swiper-slide img,article.content img"});
  }
  if ('false' == 'true') {
    stellar.plugins.heti = Object.assign({"enable":false,"css":"https://unpkg.com/heti/umd/heti.min.css","js":"https://unpkg.com/heti/umd/heti-addon.min.js"});
  }
</script>

<!-- required -->

  
<script src="/js/main.js" async></script>



<!-- optional -->



<!-- inject -->


  </div>
</body>
</html>
